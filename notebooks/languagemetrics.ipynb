{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy.stats import entropy\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from nlp_cyber_ner.dataset import read_iob2_file\n",
    "from nlp_cyber_ner.dataset import read_aptner, read_cyner, remove_leakage\n",
    "from nlp_cyber_ner.config import PROCESSED_DATA_DIR, RAW_DATA_DIR, INTERIM_DATA_DIR, TOKENPROCESSED_DATA_DIR\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will derive 3 matrices:\n",
    "- dev to dev \n",
    "- train to train\n",
    "- train (deduplicated) to dev\n",
    "\n",
    "Interpretations:\n",
    "- dev to dev: divergence between data that the model was evaluated - one from the same population as the training dataset, and another from a foreign one.\n",
    "- train to train: divergence between the data that the model saw during training, vs the training data it would've (probably ideally - reason, \n",
    "I'm saying probably is because we may have a scenario where one model ends up performing better on a foreign dev set, than the model training\n",
    "on the corresponding training set) liked to see during training. Relevant for comparison with model on the diagonal (a model that has the same train and dev).\n",
    "- train (deduplicated) to dev - difference between the data the model was trained on, and the data the model was evaluated on. Some extra care for interpretation due to deduplication: The value in cell for row i, column j corresponds to the divergence between training set of dataset i, and the dev set of dataset j, where the training set has been deduplicated against the dev set. This is why, for instance, that the diagonal is not 0, necessarily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divergences for labels - token based counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential problem here: The function is simply stripping prefixes and would count an entity of 3 different tokens with B-, I-, I- as 3 instances of that label - probably not good - should be fixed. I will create a version WITH and WITHOUT this fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity_label(label):\n",
    "    \"\"\"\n",
    "    If the label is in IOB2 format (e.g., \"B-malware\", \"I-malware\"),\n",
    "    strip off the \"B-\" or \"I-\" prefix. Otherwise, return the label as is.\n",
    "    \"\"\"\n",
    "    for prefix in [\"B-\", \"I-\"]:\n",
    "        if label.startswith(prefix):\n",
    "            return label[len(prefix):]\n",
    "    return label\n",
    "\n",
    "def get_label_distribution_ignoringprefix(data, label_set):\n",
    "    \"\"\"\n",
    "    Given data (a list of sentences, where each sentence is a tuple of lists for words and labels, respectively),\n",
    "    compute the distribution of the entity types specified in label_set.\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    for sent in data:\n",
    "        for label in sent[1]:\n",
    "            # extract the entity label (removing IOB2 prefix, if any)\n",
    "            ent_label = extract_entity_label(label)\n",
    "            if ent_label in label_set:\n",
    "                counter.update([ent_label])\n",
    "\n",
    "    distribution = np.array([counter[l] for l in label_set], dtype=np.float64)\n",
    "    # Add a tiny smoothing constant to avoid division by zero - probably not needed\n",
    "    print(distribution)\n",
    "    distribution += 1e-10\n",
    "    distribution /= distribution.sum()\n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divergence for labels - span based counts\n",
    "A given span of labels for an entity only counts towards one occurance of that underlying entity category - i.e. concecutive B-MALWARE, I-MALWARE, I-MALWARE, would count as one MALWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This entire function can probably just be removed:\n",
    "\n",
    "def extract_entity_labelandprefix(label):\n",
    "    \"\"\"\n",
    "    If the label is in IOB2 format (e.g., \"B-Malware\", \"I-Malware\"),\n",
    "    strip off the \"B-\" or \"I-\" prefix. Otherwise, return the label as is with \"O\"\n",
    "    \"\"\"\n",
    "    for prefix in [\"B-\", \"I-\"]:\n",
    "        if label.startswith(prefix):\n",
    "            relevant_prefix = prefix\n",
    "            return label[len(prefix):], relevant_prefix\n",
    "    return label, \"O\"\n",
    "\n",
    "def get_label_span_distribution(data, label_set):\n",
    "    \"\"\"\n",
    "    Given data (a list of sentences, where each sentence is a tuple of lists for words and labels, respectively),\n",
    "    compute the distribution of the entity types specified in label_set.\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    for sent in data:\n",
    "        previous_prefix = \"O\" #assuming entities don't span sentence boundaries\n",
    "        for label in sent[1]:\n",
    "            # extract the entity label (removing IOB2 prefix, if any)\n",
    "            ent_label, prefix = extract_entity_labelandprefix(label)\n",
    "            if (ent_label in label_set and prefix == \"B-\") or (ent_label in label_set and previous_prefix == \"O\"): \n",
    "                #the second case is if there happens to be an error in the annotation where the first prefix of the labeled span used prefix\n",
    "                #I rather than B - feel like we caught some of that in our manual error analysis, but maybe I'm misremembering...\n",
    "                counter.update([ent_label])\n",
    "            previous_prefix = prefix\n",
    "\n",
    "    distribution = np.array([counter[l] for l in label_set], dtype=np.float64)\n",
    "    # Add a tiny smoothing constant to avoid division by zero - probably not needed\n",
    "    distribution += 1e-10\n",
    "    distribution /= distribution.sum()\n",
    "    return distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divergence for label distributions w. prefixes\n",
    "Probably worth doing this since if datasets have vastly different lengths of their annotated entities, then this could highlight that? As an example, a malware entity that consists of 5 tokens would contribute a count of +1 to B-Malware and +4 to I-Malware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_distribution_wprefix(data):\n",
    "    \"\"\"\n",
    "    Given data (a list of sentences, where each sentence is a tuple of lists for words and labels, respectively),\n",
    "    compute the distribution of the entity types specified in label_set.\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    for sent in data:\n",
    "        for label in sent[1]:\n",
    "            if label.startswith(\"B-\") or label.startswith(\"I-\"):\n",
    "                counter.update([label])\n",
    "    temp = np.sort(list(counter.keys()))\n",
    "    distribution = np.array([counter[i] for i in temp], dtype=np.float64)\n",
    "    # Add a tiny smoothing constant to avoid division by zero - probably not needed\n",
    "    print(distribution)\n",
    "    distribution += 1e-10\n",
    "    distribution /= distribution.sum()\n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divergence for word distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the common vocabulary is just being constructed by considering all the datasets as one, and finding the 10000 most common words - there may be a better way of doing this - need to check the paper provided by Rob over email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to construct a common vocabulary across multiple datasets.\n",
    "def get_global_vocab(datasets, max_vocab_size=None):\n",
    "    \"\"\"\n",
    "    Given a list of datasets (each is a list of sentences, where each sentence \n",
    "    is a tuple (words, labels)), count the word frequencies and return the list\n",
    "    of words from the most common to the less common.\n",
    "    \"\"\"\n",
    "    global_counter = Counter()\n",
    "    for data in datasets:\n",
    "        for sent in data:\n",
    "            # sent[0] contains the words\n",
    "            global_counter.update(sent[0])\n",
    "    if max_vocab_size is not None:\n",
    "        vocab = [word for word, _ in global_counter.most_common(max_vocab_size)]\n",
    "    else:\n",
    "        vocab = list(global_counter.keys())\n",
    "    return vocab\n",
    "\n",
    "# Function to compute a word frequency distribution given a dataset and a fixed vocabulary.\n",
    "def get_word_distribution(data, vocab):\n",
    "    \"\"\"\n",
    "    Given data (a list of sentences, each sentence is a tuple (words, labels))\n",
    "    and a fixed common vocabulary, compute a normalized frequency distribution\n",
    "    of the words in 'vocab' for this dataset.\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    for sent in data:\n",
    "        counter.update(sent[0])\n",
    "    # Create a vector in the same order as the vocabulary.\n",
    "    distribution = np.array([counter[word] for word in vocab], dtype=np.float64)\n",
    "    # Smoothing to avoid zero entries\n",
    "    distribution += 1e-10\n",
    "    # Normalize to form a probability distribution\n",
    "    distribution /= distribution.sum()\n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divergence for POS distribution\n",
    "Using some off-the-shelf model for this to make predictions for the labels. Obviously we have no way of knowing the accuracy on the datasets so we're just gonna assume that the model is equally shit / good on each dataset - should be fine given we also have other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n",
      "\u001b[1m\n",
      "================= Installed pipeline packages (spaCy v3.8.2) =================\u001b[0m\n",
      "\u001b[38;5;4mℹ spaCy installation:\n",
      "/opt/anaconda3/envs/nlpproject_py312/lib/python3.12/site-packages/spacy\u001b[0m\n",
      "\n",
      "NAME             SPACY            VERSION                            \n",
      "en_core_web_sm   >=3.8.0,<3.9.0   \u001b[38;5;2m3.8.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#https://huggingface.co/spacy/en_core_web_sm/tree/main\n",
    "#https://spacy.io/api/attributeruler, https://github.com/explosion/spaCy/issues/5637\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_distributions(data):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    #https://stackoverflow.com/a/71491525/24251578\n",
    "    #I cant find the actual mapping from tags to more coarse POS tags that we're using here. \n",
    "    #Under the model's label scheme, you can only find the more fine-grained pos tags: https://spacy.io/models/en#en_core_web_sm.\n",
    "    #There is a list of more coarse POS tags here, but they don't include the full output of this model: https://spacy.io/usage/linguistic-features#pos-tagging\n",
    "\n",
    "    #Seems you can find it here but I ain't reading all that.\n",
    "        #attr_ruler = nlp.get_pipe(\"attribute_ruler\")\n",
    "        #pprint.pprint(attr_ruler.patterns)\n",
    "        #We'll just assume that each dataset ends up with the same set of labels used - which seems to be the case. The only reason this is \n",
    "        #important is because the distribution for JS needs to be the same dimension.\n",
    "    counter = Counter()\n",
    "    for sent in data:\n",
    "        doc = Doc(nlp.vocab, sent[0])\n",
    "        for token in nlp(doc):\n",
    "            counter.update([token.pos_])\n",
    "    temp = np.sort(list(counter.keys()))\n",
    "    print(temp)\n",
    "    distribution = np.array([counter[i] for i in temp], dtype=np.float64)\n",
    "    # Add a tiny smoothing constant to avoid division by zero - probably not needed\n",
    "    print(distribution)\n",
    "    distribution += 1e-10\n",
    "    distribution /= distribution.sum()\n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create one for span length distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spanlength_distribution(data):\n",
    "    \"\"\"\n",
    "    Given data (a list of sentences, where each sentence is a tuple of lists for words and labels, respectively),\n",
    "    compute the distribution of the labeled span lengths for entities in the dataset.\n",
    "    \"\"\"\n",
    "    prefixes = [\"B-\", \"I-\"]\n",
    "    counter = Counter({\"1\":0, \"2\":0, \"3\":0, \"4\":0, \"5\":0, \"length>5\":0})\n",
    "    for sent in data:\n",
    "        previous_prefix = \"O\"\n",
    "        length = 0 #this is assuming that spans don't go across sentence boundaries - which must be true?\n",
    "        for label in sent[1]:\n",
    "            if (label.startswith(\"B-\")) or (label.startswith(\"I-\") and previous_prefix == \"O\"):\n",
    "                for prefix in prefixes:\n",
    "                    if label.startswith(prefix):\n",
    "                        previous_prefix = prefix\n",
    "                        break\n",
    "                if length > 5:\n",
    "                    counter[\"length>5\"] += 1\n",
    "                    length = 1\n",
    "                    continue\n",
    "                elif length >= 1:\n",
    "                    counter[str(length)] += 1\n",
    "                    length = 1\n",
    "                    continue\n",
    "            elif (label == \"O\" and previous_prefix in prefixes):\n",
    "                if length > 5:\n",
    "                    counter[\"length>5\"] += 1\n",
    "                    length = 0\n",
    "                    continue\n",
    "                elif length >= 1:\n",
    "                    counter[str(length)] += 1\n",
    "                    length = 0\n",
    "                    continue              \n",
    "            elif label.startswith(\"I-\") and previous_prefix != \"O\":\n",
    "                previous_prefix = \"I-\"\n",
    "                length += 1   \n",
    "    temp = np.sort(list(counter.keys()))\n",
    "    distribution = np.array([counter[i] for i in temp], dtype=np.float64)\n",
    "    # Add a tiny smoothing constant to avoid division by zero - probably not needed\n",
    "    distribution += 1e-10\n",
    "    distribution /= distribution.sum()\n",
    "    return distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute JS and KL divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_divergence_matrix(distributions: dict):\n",
    "    dataset_names = list(distributions.keys())\n",
    "    n_datasets = len(dataset_names)\n",
    "\n",
    "    #matrices for kuhl and jensen\n",
    "    js_matrix = np.zeros((n_datasets, n_datasets))\n",
    "    kl_matrix = np.zeros((n_datasets, n_datasets))\n",
    "\n",
    "\n",
    "    # For KL divergence, we compute the symmetric version: 0.5*(KL(P||Q) + KL(Q||P)). If not, the distance between two pairs of datasets may not\n",
    "    # be the same depending on the 'perspective'\n",
    "    for i in range(n_datasets):\n",
    "        for j in range(n_datasets):\n",
    "            P = distributions[dataset_names[i]]\n",
    "            Q = distributions[dataset_names[j]]\n",
    "            # Jensen-Shannon distance using SciPy (this is already symmetric)\n",
    "            js_matrix[i, j] = jensenshannon(P, Q)\n",
    "            # Symmetric KL divergence\n",
    "            kl_sym = 0.5 * (entropy(P, Q) + entropy(Q, P))\n",
    "            kl_matrix[i, j] = kl_sym\n",
    "    return js_matrix, kl_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnrti_path = PROCESSED_DATA_DIR / \"dnrti\"\n",
    "dnrti_train_path = dnrti_path / \"train.unified\"\n",
    "dnrti_train_data = read_iob2_file(dnrti_train_path, word_index=0, tag_index=1)\n",
    "dnrti_dev_path = dnrti_path / \"valid.unified\"\n",
    "dnrti_dev_data = read_iob2_file(dnrti_dev_path)\n",
    "\n",
    "attacker_path = PROCESSED_DATA_DIR / \"attacker\"\n",
    "attacker_train_path  = attacker_path / \"train.unified\"\n",
    "attacker_train_data = read_iob2_file(attacker_train_path, word_index=0, tag_index=1)\n",
    "attacker_dev_path = attacker_path / \"valid.unified\"\n",
    "attacker_dev_data = read_iob2_file(attacker_dev_path)\n",
    "\n",
    "aptner_path = PROCESSED_DATA_DIR / \"APTNer\"\n",
    "aptner_train_path = aptner_path / \"train.unified\"\n",
    "aptner_train_data = read_iob2_file(aptner_train_path)\n",
    "aptner_dev_path = aptner_path / \"valid.unified\"\n",
    "aptner_dev_data = read_iob2_file(aptner_dev_path)\n",
    "\n",
    "cyner_path = PROCESSED_DATA_DIR / \"cyner\"\n",
    "cyner_train_path = cyner_path / \"train.unified\"\n",
    "cyner_train_data = read_iob2_file(cyner_train_path)\n",
    "cyner_dev_path = cyner_path / \"valid.unified\"\n",
    "cyner_dev_data = read_iob2_file(cyner_dev_path)\n",
    "\n",
    "#mismatched distribution dimensions?\n",
    "#some of these functions don't have a predefined set of categories, but if it does happen that some datasets miss certain values, it'll cause\n",
    "#errors during the JS calculations anyway - It can't be different dimensions, and it doens't seem to be the case that any of the datasets are \n",
    "#completely missing a certain category - can be fixed by just defining a fixed category set, but cba, since it all works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===========================================\n",
      "=== DISTRIBUTION METHOD: IgnoringPrefix ===\n",
      "===========================================\n",
      "[1777. 3835. 7937. 3219.]\n",
      "[ 901. 1886. 2608.  914.]\n",
      "[4252.  232. 6135.  516.]\n",
      "[ 898. 1321.  400.   90.]\n",
      "\n",
      "--- Train-to-Train ---\n",
      "Jensen-Shannon Distance Matrix:\n",
      "[[0.         0.07986706 0.34266991 0.36096433]\n",
      " [0.07986706 0.         0.34469139 0.28697856]\n",
      " [0.34266991 0.34469139 0.         0.44287795]\n",
      " [0.36096433 0.28697856 0.44287795 0.        ]]\n",
      "\n",
      "Symmetric KL Divergence Matrix:\n",
      "[[0.         0.02558817 0.53522315 0.55553726]\n",
      " [0.02558817 0.         0.56385104 0.34459174]\n",
      " [0.53522315 0.56385104 0.         1.00717131]\n",
      " [0.55553726 0.34459174 1.00717131 0.        ]]\n",
      "[280. 473. 990. 388.]\n",
      "[133. 301. 331. 151.]\n",
      "[444.  81. 600.  19.]\n",
      "[265. 273. 162.  14.]\n",
      "\n",
      "--- Dev-to-Dev ---\n",
      "Jensen-Shannon Distance Matrix:\n",
      "[[0.         0.09389453 0.30797194 0.31487711]\n",
      " [0.09389453 0.         0.34131368 0.25890805]\n",
      " [0.30797194 0.34131368 0.         0.29883119]\n",
      " [0.31487711 0.25890805 0.29883119 0.        ]]\n",
      "\n",
      "Symmetric KL Divergence Matrix:\n",
      "[[0.         0.03543253 0.42711739 0.43428761]\n",
      " [0.03543253 0.         0.51776628 0.29600349]\n",
      " [0.42711739 0.51776628 0.         0.3880076 ]\n",
      " [0.43428761 0.29600349 0.3880076  0.        ]]\n",
      "Removed 500 leaked sentences from training data.\n",
      "[1300. 3633. 7485. 2626.]\n",
      "[280. 473. 990. 388.]\n",
      "Removed 0 leaked sentences from training data.\n",
      "[1777. 3835. 7937. 3219.]\n",
      "[133. 301. 331. 151.]\n",
      "Removed 46 leaked sentences from training data.\n",
      "[1775. 3814. 7841. 3200.]\n",
      "[444.  81. 600.  19.]\n",
      "Removed 0 leaked sentences from training data.\n",
      "[1777. 3835. 7937. 3219.]\n",
      "[265. 273. 162.  14.]\n",
      "Removed 0 leaked sentences from training data.\n",
      "[ 901. 1886. 2608.  914.]\n",
      "[280. 473. 990. 388.]\n",
      "Removed 207 leaked sentences from training data.\n",
      "[ 781. 1619. 2299.  805.]\n",
      "[133. 301. 331. 151.]\n",
      "Removed 0 leaked sentences from training data.\n",
      "[ 901. 1886. 2608.  914.]\n",
      "[444.  81. 600.  19.]\n",
      "Removed 0 leaked sentences from training data.\n",
      "[ 901. 1886. 2608.  914.]\n",
      "[265. 273. 162.  14.]\n",
      "Removed 286 leaked sentences from training data.\n",
      "[4144.  214. 5934.  457.]\n",
      "[280. 473. 990. 388.]\n",
      "Removed 0 leaked sentences from training data.\n",
      "[4252.  232. 6135.  516.]\n",
      "[133. 301. 331. 151.]\n",
      "Removed 71 leaked sentences from training data.\n",
      "[4227.  225. 6107.  512.]\n",
      "[444.  81. 600.  19.]\n",
      "Removed 1 leaked sentences from training data.\n",
      "[4252.  232. 6135.  516.]\n",
      "[265. 273. 162.  14.]\n",
      "Removed 0 leaked sentences from training data.\n",
      "[ 898. 1321.  400.   90.]\n",
      "[280. 473. 990. 388.]\n",
      "Removed 0 leaked sentences from training data.\n",
      "[ 898. 1321.  400.   90.]\n",
      "[133. 301. 331. 151.]\n",
      "Removed 0 leaked sentences from training data.\n",
      "[ 898. 1321.  400.   90.]\n",
      "[444.  81. 600.  19.]\n",
      "Removed 25 leaked sentences from training data.\n",
      "[ 898. 1321.  400.   90.]\n",
      "[265. 273. 162.  14.]\n",
      "\n",
      "--- Train(deduplicated)-to-Dev ---\n",
      "Jensen-Shannon Distance Matrix:\n",
      "[[0.05354488 0.10159504 0.32967353 0.33408855]\n",
      " [0.06914041 0.04223393 0.31592283 0.26375104]\n",
      " [0.32756907 0.36976507 0.10593437 0.3692673 ]\n",
      " [0.34529086 0.27108238 0.3819674  0.09414017]]\n",
      "\n",
      "Symmetric KL Divergence Matrix:\n",
      "[[0.01153706 0.04145772 0.49296368 0.49289361]\n",
      " [0.01917479 0.00714002 0.43904301 0.30049962]\n",
      " [0.48942613 0.6540482  0.04741653 0.68136262]\n",
      " [0.50537305 0.30932209 0.6512033  0.03565283]]\n",
      "===========================================================\n",
      "\n",
      "\n",
      "===========================================\n",
      "=== DISTRIBUTION METHOD: SpanBased ===\n",
      "===========================================\n",
      "\n",
      "--- Train-to-Train ---\n",
      "Jensen-Shannon Distance Matrix:\n",
      "[[0.         0.13617656 0.31196765 0.3691458 ]\n",
      " [0.13617656 0.         0.28220034 0.27089175]\n",
      " [0.31196765 0.28220034 0.         0.42346006]\n",
      " [0.3691458  0.27089175 0.42346006 0.        ]]\n",
      "\n",
      "Symmetric KL Divergence Matrix:\n",
      "[[0.         0.07614166 0.43688426 0.58493091]\n",
      " [0.07614166 0.         0.37981549 0.30479624]\n",
      " [0.43688426 0.37981549 0.         0.88910906]\n",
      " [0.58493091 0.30479624 0.88910906 0.        ]]\n",
      "\n",
      "--- Dev-to-Dev ---\n",
      "Jensen-Shannon Distance Matrix:\n",
      "[[0.         0.1177363  0.29543219 0.36468549]\n",
      " [0.1177363  0.         0.25518505 0.2925345 ]\n",
      " [0.29543219 0.25518505 0.         0.27911849]\n",
      " [0.36468549 0.2925345  0.27911849 0.        ]]\n",
      "\n",
      "Symmetric KL Divergence Matrix:\n",
      "[[0.         0.05676031 0.38686036 0.5824755 ]\n",
      " [0.05676031 0.         0.27376642 0.35868638]\n",
      " [0.38686036 0.27376642 0.         0.3299068 ]\n",
      " [0.5824755  0.35868638 0.3299068  0.        ]]\n",
      "Removed 500 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 46 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 207 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 286 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 71 leaked sentences from training data.\n",
      "Removed 1 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 25 leaked sentences from training data.\n",
      "\n",
      "--- Train(deduplicated)-to-Dev ---\n",
      "Jensen-Shannon Distance Matrix:\n",
      "[[0.04685495 0.12868213 0.3132396  0.38174753]\n",
      " [0.1224758  0.02629667 0.23711523 0.27958455]\n",
      " [0.30247096 0.29710179 0.13268362 0.37969063]\n",
      " [0.35462154 0.2792152  0.33235349 0.08494395]]\n",
      "\n",
      "Symmetric KL Divergence Matrix:\n",
      "[[0.00881604 0.06788617 0.43802178 0.64392931]\n",
      " [0.06149291 0.00276825 0.23520263 0.32598835]\n",
      " [0.41167295 0.42170814 0.07445009 0.68091531]\n",
      " [0.53662785 0.32455711 0.4794519  0.02894749]]\n",
      "===========================================================\n",
      "\n",
      "\n",
      "===========================================\n",
      "=== DISTRIBUTION METHOD: WithPrefix ===\n",
      "===========================================\n",
      "[1221. 5529. 2449. 1896.  556. 2408. 1386. 1323.]\n",
      "[ 548. 1390.  822.  197.  353. 1218. 1064.  717.]\n",
      "[3109. 4742.  205.  483. 1143. 1393.   27.   33.]\n",
      "[703. 284. 837.  48. 195. 116. 484.  42.]\n",
      "\n",
      "--- Train-to-Train ---\n",
      "Jensen-Shannon Distance Matrix:\n",
      "[[0.         0.17526216 0.35662361 0.36246624]\n",
      " [0.17526216 0.         0.40123206 0.31647466]\n",
      " [0.35662361 0.40123206 0.         0.4511981 ]\n",
      " [0.36246624 0.31647466 0.4511981  0.        ]]\n",
      "\n",
      "Symmetric KL Divergence Matrix:\n",
      "[[0.         0.1271366  0.62326889 0.56148289]\n",
      " [0.1271366  0.         0.87478338 0.43151897]\n",
      " [0.62326889 0.87478338 0.         1.08575745]\n",
      " [0.56148289 0.43151897 1.08575745 0.        ]]\n",
      "[191. 715. 317. 240.  89. 275. 156. 148.]\n",
      "[ 67. 194. 119.  28.  66. 137. 182. 123.]\n",
      "[376. 434.  80.  16.  68. 166.   1.   3.]\n",
      "[254.  92. 182.   9.  11.  70.  91.   5.]\n",
      "\n",
      "--- Dev-to-Dev ---\n",
      "Jensen-Shannon Distance Matrix:\n",
      "[[0.         0.20736572 0.33133543 0.34510501]\n",
      " [0.20736572 0.         0.41202332 0.34592461]\n",
      " [0.33133543 0.41202332 0.         0.33482917]\n",
      " [0.34510501 0.34592461 0.33482917 0.        ]]\n",
      "\n",
      "Symmetric KL Divergence Matrix:\n",
      "[[0.         0.17871046 0.56430059 0.5244984 ]\n",
      " [0.17871046 0.         1.06295504 0.55254076]\n",
      " [0.56430059 1.06295504 0.         0.61245039]\n",
      " [0.5244984  0.55254076 0.61245039 0.        ]]\n",
      "Removed 500 leaked sentences from training data.\n",
      "[ 918. 5206. 2322. 1525.  382. 2279. 1311. 1101.]\n",
      "[191. 715. 317. 240.  89. 275. 156. 148.]\n",
      "Removed 0 leaked sentences from training data.\n",
      "[1221. 5529. 2449. 1896.  556. 2408. 1386. 1323.]\n",
      "[ 67. 194. 119.  28.  66. 137. 182. 123.]\n",
      "Removed 46 leaked sentences from training data.\n",
      "[1219. 5469. 2431. 1883.  556. 2372. 1383. 1317.]\n",
      "[376. 434.  80.  16.  68. 166.   1.   3.]\n",
      "Removed 0 leaked sentences from training data.\n",
      "[1221. 5529. 2449. 1896.  556. 2408. 1386. 1323.]\n",
      "[254.  92. 182.   9.  11.  70.  91.   5.]\n",
      "Removed 0 leaked sentences from training data.\n",
      "[ 548. 1390.  822.  197.  353. 1218. 1064.  717.]\n",
      "[191. 715. 317. 240.  89. 275. 156. 148.]\n",
      "Removed 207 leaked sentences from training data.\n",
      "[ 497. 1216.  726.  160.  284. 1083.  893.  645.]\n",
      "[ 67. 194. 119.  28.  66. 137. 182. 123.]\n",
      "Removed 0 leaked sentences from training data.\n",
      "[ 548. 1390.  822.  197.  353. 1218. 1064.  717.]\n",
      "[376. 434.  80.  16.  68. 166.   1.   3.]\n",
      "Removed 0 leaked sentences from training data.\n",
      "[ 548. 1390.  822.  197.  353. 1218. 1064.  717.]\n",
      "[254.  92. 182.   9.  11.  70.  91.   5.]\n",
      "Removed 286 leaked sentences from training data.\n",
      "[3035. 4579.  187.  427. 1109. 1355.   27.   30.]\n",
      "[191. 715. 317. 240.  89. 275. 156. 148.]\n",
      "Removed 0 leaked sentences from training data.\n",
      "[3109. 4742.  205.  483. 1143. 1393.   27.   33.]\n",
      "[ 67. 194. 119.  28.  66. 137. 182. 123.]\n",
      "Removed 71 leaked sentences from training data.\n",
      "[3087. 4720.  201.  480. 1140. 1387.   24.   32.]\n",
      "[376. 434.  80.  16.  68. 166.   1.   3.]\n",
      "Removed 1 leaked sentences from training data.\n",
      "[3109. 4742.  205.  483. 1143. 1393.   27.   33.]\n",
      "[254.  92. 182.   9.  11.  70.  91.   5.]\n",
      "Removed 0 leaked sentences from training data.\n",
      "[703. 284. 837.  48. 195. 116. 484.  42.]\n",
      "[191. 715. 317. 240.  89. 275. 156. 148.]\n",
      "Removed 0 leaked sentences from training data.\n",
      "[703. 284. 837.  48. 195. 116. 484.  42.]\n",
      "[ 67. 194. 119.  28.  66. 137. 182. 123.]\n",
      "Removed 0 leaked sentences from training data.\n",
      "[703. 284. 837.  48. 195. 116. 484.  42.]\n",
      "[376. 434.  80.  16.  68. 166.   1.   3.]\n",
      "Removed 25 leaked sentences from training data.\n",
      "[703. 284. 837.  48. 195. 116. 484.  42.]\n",
      "[254.  92. 182.   9.  11.  70.  91.   5.]\n",
      "\n",
      "--- Train(deduplicated)-to-Dev ---\n",
      "Jensen-Shannon Distance Matrix:\n",
      "[[0.05801686 0.19746368 0.35325099 0.35786743]\n",
      " [0.18612717 0.06471221 0.38242566 0.32570309]\n",
      " [0.33809908 0.42432434 0.13128673 0.40985553]\n",
      " [0.34774526 0.3154299  0.40147882 0.15615116]]\n",
      "\n",
      "Symmetric KL Divergence Matrix:\n",
      "[[0.01354271 0.16183411 0.65248715 0.57155545]\n",
      " [0.14337967 0.01679723 0.88743652 0.47704894]\n",
      " [0.55443361 1.01431812 0.0723788  0.85200446]\n",
      " [0.51406981 0.43269824 0.91071757 0.10223415]]\n",
      "===========================================================\n",
      "\n",
      "\n",
      "===========================================\n",
      "=== DISTRIBUTION METHOD: WordDist ===\n",
      "===========================================\n",
      "\n",
      "--- Train-to-Train ---\n",
      "Jensen-Shannon Distance Matrix:\n",
      "[[0.         0.35760725 0.13831401 0.40274952]\n",
      " [0.35760725 0.         0.3357986  0.37419193]\n",
      " [0.13831401 0.3357986  0.         0.36833671]\n",
      " [0.40274952 0.37419193 0.36833671 0.        ]]\n",
      "\n",
      "Symmetric KL Divergence Matrix:\n",
      "[[0.         2.86754119 0.32412469 3.30223229]\n",
      " [2.86754119 0.         2.46597506 3.15551478]\n",
      " [0.32412469 2.46597506 0.         2.61344688]\n",
      " [3.30223229 3.15551478 2.61344688 0.        ]]\n",
      "\n",
      "--- Dev-to-Dev ---\n",
      "Jensen-Shannon Distance Matrix:\n",
      "[[0.         0.43331349 0.37307906 0.4707207 ]\n",
      " [0.43331349 0.         0.40500397 0.46921746]\n",
      " [0.37307906 0.40500397 0.         0.41378029]\n",
      " [0.4707207  0.46921746 0.41378029 0.        ]]\n",
      "\n",
      "Symmetric KL Divergence Matrix:\n",
      "[[0.         5.28401586 3.61465548 5.94851187]\n",
      " [5.28401586 0.         4.36613438 6.16659823]\n",
      " [3.61465548 4.36613438 0.         4.47241725]\n",
      " [5.94851187 6.16659823 4.47241725 0.        ]]\n",
      "Removed 500 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 46 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 207 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 286 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 71 leaked sentences from training data.\n",
      "Removed 1 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 25 leaked sentences from training data.\n",
      "\n",
      "--- Train(deduplicated)-to-Dev ---\n",
      "Jensen-Shannon Distance Matrix:\n",
      "[[0.31258369 0.41659888 0.32964615 0.45355186]\n",
      " [0.38949561 0.34087477 0.34555944 0.42380335]\n",
      " [0.31578788 0.40334397 0.29630735 0.42663962]\n",
      " [0.43080244 0.43586832 0.36634973 0.30713639]]\n",
      "\n",
      "Symmetric KL Divergence Matrix:\n",
      "[[2.37480618 4.20415643 2.41297445 4.62927543]\n",
      " [3.90758582 2.89750329 2.97269921 4.29816212]\n",
      " [2.29500266 3.78459083 1.91280747 3.87598426]\n",
      " [4.6078584  4.854387   3.36104378 2.31597525]]\n",
      "===========================================================\n",
      "\n",
      "\n",
      "===========================================\n",
      "=== DISTRIBUTION METHOD: POSDist ===\n",
      "===========================================\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[1.0739e+04 1.4868e+04 4.4670e+03 4.8510e+03 4.1960e+03 1.2007e+04\n",
      " 5.4000e+01 3.0929e+04 2.3450e+03 3.1580e+03 4.4580e+03 1.5530e+04\n",
      " 1.4497e+04 1.6260e+03 2.2000e+01 1.6460e+04 1.3900e+02]\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[ 4417.  6712.  2040.  2845.  2011.  5722.    27. 15294.  1082.  1766.\n",
      "  2483.  5866.  7201.   913.    95.  8419.    69.]\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[1.3307e+04 1.8717e+04 5.5990e+03 6.9290e+03 5.3170e+03 1.6752e+04\n",
      " 8.9000e+01 3.9760e+04 3.4000e+03 3.7090e+03 5.9050e+03 1.9532e+04\n",
      " 2.0186e+04 2.2460e+03 3.1000e+01 2.0705e+04 4.3400e+02]\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[ 4444.  6345.  1989.  2554.  1738.  6739.    32. 15841.  1345.  1495.\n",
      "  2422.  5612.  8497.   936.    18.  7889.   295.]\n",
      "\n",
      "--- Train-to-Train ---\n",
      "Jensen-Shannon Distance Matrix:\n",
      "[[0.         0.04199404 0.02111907 0.05641533]\n",
      " [0.04199404 0.         0.04111081 0.0438426 ]\n",
      " [0.02111907 0.04111081 0.         0.04155997]\n",
      " [0.05641533 0.0438426  0.04155997 0.        ]]\n",
      "\n",
      "Symmetric KL Divergence Matrix:\n",
      "[[0.         0.00728999 0.00180301 0.01295374]\n",
      " [0.00728999 0.         0.0069864  0.00797699]\n",
      " [0.00180301 0.0069864  0.         0.00692823]\n",
      " [0.01295374 0.00797699 0.00692823 0.        ]]\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[1.252e+03 1.856e+03 5.110e+02 6.730e+02 5.110e+02 1.575e+03 8.000e+00\n",
      " 3.900e+03 2.950e+02 4.110e+02 5.200e+02 1.998e+03 1.747e+03 1.930e+02\n",
      " 1.000e+00 2.129e+03 2.200e+01]\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[ 588.  881.  245.  345.  260.  731.    4. 1926.  142.  207.  298.  728.\n",
      "  896.  103.   19. 1014.    5.]\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[2717. 3829. 1063. 1377. 1200. 3869.   41. 8630.  668.  789. 1230. 3701.\n",
      " 4595.  468.   14. 4473.  155.]\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[1100. 1848.  527.  765.  552. 2018.   14. 4375.  294.  378.  760. 1723.\n",
      " 2523.  284.    6. 2282.   81.]\n",
      "\n",
      "--- Dev-to-Dev ---\n",
      "Jensen-Shannon Distance Matrix:\n",
      "[[0.         0.04413039 0.04106488 0.0607105 ]\n",
      " [0.04413039 0.         0.04469234 0.05426046]\n",
      " [0.04106488 0.04469234 0.         0.0312925 ]\n",
      " [0.0607105  0.05426046 0.0312925  0.        ]]\n",
      "\n",
      "Symmetric KL Divergence Matrix:\n",
      "[[0.         0.00919047 0.00687539 0.01489396]\n",
      " [0.00919047 0.         0.00862162 0.0125142 ]\n",
      " [0.00687539 0.00862162 0.         0.00392231]\n",
      " [0.01489396 0.0125142  0.00392231 0.        ]]\n",
      "Removed 500 leaked sentences from training data.\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[9.7160e+03 1.3579e+04 4.1130e+03 4.4360e+03 3.7940e+03 1.0662e+04\n",
      " 4.2000e+01 2.7656e+04 2.1910e+03 2.8000e+03 4.0470e+03 1.4243e+04\n",
      " 1.3082e+04 1.5260e+03 2.2000e+01 1.4771e+04 1.1200e+02]\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[1.252e+03 1.856e+03 5.110e+02 6.730e+02 5.110e+02 1.575e+03 8.000e+00\n",
      " 3.900e+03 2.950e+02 4.110e+02 5.200e+02 1.998e+03 1.747e+03 1.930e+02\n",
      " 1.000e+00 2.129e+03 2.200e+01]\n",
      "Removed 0 leaked sentences from training data.\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[1.0739e+04 1.4868e+04 4.4670e+03 4.8510e+03 4.1960e+03 1.2007e+04\n",
      " 5.4000e+01 3.0929e+04 2.3450e+03 3.1580e+03 4.4580e+03 1.5530e+04\n",
      " 1.4497e+04 1.6260e+03 2.2000e+01 1.6460e+04 1.3900e+02]\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[ 588.  881.  245.  345.  260.  731.    4. 1926.  142.  207.  298.  728.\n",
      "  896.  103.   19. 1014.    5.]\n",
      "Removed 46 leaked sentences from training data.\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[1.0607e+04 1.4733e+04 4.4290e+03 4.7980e+03 4.1350e+03 1.1894e+04\n",
      " 5.4000e+01 3.0629e+04 2.3150e+03 3.1360e+03 4.4360e+03 1.5399e+04\n",
      " 1.4336e+04 1.6120e+03 2.2000e+01 1.6339e+04 1.3900e+02]\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[2717. 3829. 1063. 1377. 1200. 3869.   41. 8630.  668.  789. 1230. 3701.\n",
      " 4595.  468.   14. 4473.  155.]\n",
      "Removed 0 leaked sentences from training data.\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[1.0739e+04 1.4868e+04 4.4670e+03 4.8510e+03 4.1960e+03 1.2007e+04\n",
      " 5.4000e+01 3.0929e+04 2.3450e+03 3.1580e+03 4.4580e+03 1.5530e+04\n",
      " 1.4497e+04 1.6260e+03 2.2000e+01 1.6460e+04 1.3900e+02]\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[1100. 1848.  527.  765.  552. 2018.   14. 4375.  294.  378.  760. 1723.\n",
      " 2523.  284.    6. 2282.   81.]\n",
      "Removed 0 leaked sentences from training data.\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[ 4417.  6712.  2040.  2845.  2011.  5722.    27. 15294.  1082.  1766.\n",
      "  2483.  5866.  7201.   913.    95.  8419.    69.]\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[1.252e+03 1.856e+03 5.110e+02 6.730e+02 5.110e+02 1.575e+03 8.000e+00\n",
      " 3.900e+03 2.950e+02 4.110e+02 5.200e+02 1.998e+03 1.747e+03 1.930e+02\n",
      " 1.000e+00 2.129e+03 2.200e+01]\n",
      "Removed 207 leaked sentences from training data.\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[ 3969.  6037.  1869.  2610.  1818.  5170.    23. 13848.   965.  1608.\n",
      "  2279.  5184.  6490.   840.    87.  7583.    66.]\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[ 588.  881.  245.  345.  260.  731.    4. 1926.  142.  207.  298.  728.\n",
      "  896.  103.   19. 1014.    5.]\n",
      "Removed 0 leaked sentences from training data.\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[ 4417.  6712.  2040.  2845.  2011.  5722.    27. 15294.  1082.  1766.\n",
      "  2483.  5866.  7201.   913.    95.  8419.    69.]\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[2717. 3829. 1063. 1377. 1200. 3869.   41. 8630.  668.  789. 1230. 3701.\n",
      " 4595.  468.   14. 4473.  155.]\n",
      "Removed 0 leaked sentences from training data.\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[ 4417.  6712.  2040.  2845.  2011.  5722.    27. 15294.  1082.  1766.\n",
      "  2483.  5866.  7201.   913.    95.  8419.    69.]\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[1100. 1848.  527.  765.  552. 2018.   14. 4375.  294.  378.  760. 1723.\n",
      " 2523.  284.    6. 2282.   81.]\n",
      "Removed 286 leaked sentences from training data.\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[1.2721e+04 1.7980e+04 5.3780e+03 6.7110e+03 5.0710e+03 1.5995e+04\n",
      " 8.2000e+01 3.7892e+04 3.2950e+03 3.5010e+03 5.6550e+03 1.8728e+04\n",
      " 1.9356e+04 2.1890e+03 3.1000e+01 1.9733e+04 4.1900e+02]\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[1.252e+03 1.856e+03 5.110e+02 6.730e+02 5.110e+02 1.575e+03 8.000e+00\n",
      " 3.900e+03 2.950e+02 4.110e+02 5.200e+02 1.998e+03 1.747e+03 1.930e+02\n",
      " 1.000e+00 2.129e+03 2.200e+01]\n",
      "Removed 0 leaked sentences from training data.\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[1.3307e+04 1.8717e+04 5.5990e+03 6.9290e+03 5.3170e+03 1.6752e+04\n",
      " 8.9000e+01 3.9760e+04 3.4000e+03 3.7090e+03 5.9050e+03 1.9532e+04\n",
      " 2.0186e+04 2.2460e+03 3.1000e+01 2.0705e+04 4.3400e+02]\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[ 588.  881.  245.  345.  260.  731.    4. 1926.  142.  207.  298.  728.\n",
      "  896.  103.   19. 1014.    5.]\n",
      "Removed 71 leaked sentences from training data.\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[1.3128e+04 1.8506e+04 5.5350e+03 6.8610e+03 5.2200e+03 1.6542e+04\n",
      " 8.8000e+01 3.9257e+04 3.3700e+03 3.6630e+03 5.8390e+03 1.9405e+04\n",
      " 1.9949e+04 2.2200e+03 3.1000e+01 2.0449e+04 4.3400e+02]\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[2717. 3829. 1063. 1377. 1200. 3869.   41. 8630.  668.  789. 1230. 3701.\n",
      " 4595.  468.   14. 4473.  155.]\n",
      "Removed 1 leaked sentences from training data.\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[1.3307e+04 1.8717e+04 5.5990e+03 6.9290e+03 5.3170e+03 1.6752e+04\n",
      " 8.9000e+01 3.9760e+04 3.4000e+03 3.7090e+03 5.9050e+03 1.9532e+04\n",
      " 2.0185e+04 2.2460e+03 3.1000e+01 2.0705e+04 4.3400e+02]\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[1100. 1848.  527.  765.  552. 2018.   14. 4375.  294.  378.  760. 1723.\n",
      " 2523.  284.    6. 2282.   81.]\n",
      "Removed 0 leaked sentences from training data.\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[ 4444.  6345.  1989.  2554.  1738.  6739.    32. 15841.  1345.  1495.\n",
      "  2422.  5612.  8497.   936.    18.  7889.   295.]\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[1.252e+03 1.856e+03 5.110e+02 6.730e+02 5.110e+02 1.575e+03 8.000e+00\n",
      " 3.900e+03 2.950e+02 4.110e+02 5.200e+02 1.998e+03 1.747e+03 1.930e+02\n",
      " 1.000e+00 2.129e+03 2.200e+01]\n",
      "Removed 0 leaked sentences from training data.\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[ 4444.  6345.  1989.  2554.  1738.  6739.    32. 15841.  1345.  1495.\n",
      "  2422.  5612.  8497.   936.    18.  7889.   295.]\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[ 588.  881.  245.  345.  260.  731.    4. 1926.  142.  207.  298.  728.\n",
      "  896.  103.   19. 1014.    5.]\n",
      "Removed 0 leaked sentences from training data.\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[ 4444.  6345.  1989.  2554.  1738.  6739.    32. 15841.  1345.  1495.\n",
      "  2422.  5612.  8497.   936.    18.  7889.   295.]\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[2717. 3829. 1063. 1377. 1200. 3869.   41. 8630.  668.  789. 1230. 3701.\n",
      " 4595.  468.   14. 4473.  155.]\n",
      "Removed 25 leaked sentences from training data.\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[ 4444.  6345.  1989.  2554.  1738.  6739.    32. 15821.  1325.  1495.\n",
      "  2422.  5612.  8471.   936.    18.  7888.   295.]\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n",
      "[1100. 1848.  527.  765.  552. 2018.   14. 4375.  294.  378.  760. 1723.\n",
      " 2523.  284.    6. 2282.   81.]\n",
      "\n",
      "--- Train(deduplicated)-to-Dev ---\n",
      "Jensen-Shannon Distance Matrix:\n",
      "[[0.01770765 0.04133073 0.04087191 0.06165252]\n",
      " [0.04329741 0.01661484 0.04359336 0.0485763 ]\n",
      " [0.02402037 0.04209156 0.02560897 0.04625693]\n",
      " [0.05716238 0.0485405  0.02797081 0.02464628]]\n",
      "\n",
      "Symmetric KL Divergence Matrix:\n",
      "[[0.0012581  0.00746096 0.00685219 0.01541122]\n",
      " [0.00813797 0.00110822 0.0078107  0.00968479]\n",
      " [0.00231872 0.00775144 0.00263516 0.00857853]\n",
      " [0.01322847 0.01027207 0.00313798 0.00243258]]\n",
      "===========================================================\n",
      "\n",
      "\n",
      "===========================================\n",
      "=== DISTRIBUTION METHOD: SpanLengthDist ===\n",
      "===========================================\n",
      "\n",
      "--- Train-to-Train ---\n",
      "Jensen-Shannon Distance Matrix:\n",
      "[[0.         0.22936515 0.03539619 0.05009848]\n",
      " [0.22936515 0.         0.24052952 0.19019921]\n",
      " [0.03539619 0.24052952 0.         0.06866643]\n",
      " [0.05009848 0.19019921 0.06866643 0.        ]]\n",
      "\n",
      "Symmetric KL Divergence Matrix:\n",
      "[[0.         0.26270405 0.00509496 0.01044159]\n",
      " [0.26270405 0.         0.31248292 0.16354544]\n",
      " [0.00509496 0.31248292 0.         0.01984823]\n",
      " [0.01044159 0.16354544 0.01984823 0.        ]]\n",
      "\n",
      "--- Dev-to-Dev ---\n",
      "Jensen-Shannon Distance Matrix:\n",
      "[[0.         0.28793086 0.12249062 0.13124202]\n",
      " [0.28793086 0.         0.34799242 0.21926447]\n",
      " [0.12249062 0.34799242 0.         0.15046813]\n",
      " [0.13124202 0.21926447 0.15046813 0.        ]]\n",
      "\n",
      "Symmetric KL Divergence Matrix:\n",
      "[[0.         0.88962542 0.11451862 0.46279672]\n",
      " [0.88962542 0.         0.8020282  0.19822299]\n",
      " [0.11451862 0.8020282  0.         0.2797514 ]\n",
      " [0.46279672 0.19822299 0.2797514  0.        ]]\n",
      "Removed 500 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 46 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 207 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 286 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 71 leaked sentences from training data.\n",
      "Removed 1 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 25 leaked sentences from training data.\n",
      "\n",
      "--- Train(deduplicated)-to-Dev ---\n",
      "Jensen-Shannon Distance Matrix:\n",
      "[[0.0754726  0.25898799 0.14830318 0.10556701]\n",
      " [0.26480789 0.07800091 0.31936959 0.18108133]\n",
      " [0.08458899 0.27316095 0.13981647 0.10533993]\n",
      " [0.10519283 0.22426471 0.17647083 0.08704456]]\n",
      "\n",
      "Symmetric KL Divergence Matrix:\n",
      "[[0.16980204 0.3483377  0.11222577 0.05416863]\n",
      " [1.16205431 0.02458686 0.85702472 0.13339517]\n",
      " [0.20248189 0.43260172 0.13043904 0.06378875]\n",
      " [0.30332294 0.23889539 0.21647758 0.0321578 ]]\n",
      "===========================================================\n"
     ]
    }
   ],
   "source": [
    "train_datasets = [\n",
    "    (\"DNRTI\", dnrti_train_data),\n",
    "    (\"Attacker\", attacker_train_data),\n",
    "    (\"APTNER\", aptner_train_data),\n",
    "    (\"CyNER\", cyner_train_data)\n",
    "]\n",
    "\n",
    "dev_datasets = [\n",
    "    (\"DNRTI\", dnrti_dev_data),\n",
    "    (\"Attacker\", attacker_dev_data),\n",
    "    (\"APTNER\", aptner_dev_data),\n",
    "    (\"CyNER\", cyner_dev_data)\n",
    "]\n",
    "\n",
    "LABEL_SET = [\"Malware\", \"System\", \"Organization\", \"Vulnerability\"]\n",
    "\n",
    "all_datasets = [dnrti_train_data, attacker_train_data, aptner_train_data, cyner_train_data, dnrti_dev_data, attacker_dev_data, aptner_dev_data, cyner_dev_data]\n",
    "common_vocab = get_global_vocab(all_datasets, max_vocab_size=10000)\n",
    "\n",
    "distribution_methods = {\n",
    "    \"IgnoringPrefix\":  lambda data: get_label_distribution_ignoringprefix(data, LABEL_SET),\n",
    "    \"SpanBased\":       lambda data: get_label_span_distribution(data, LABEL_SET),\n",
    "    \"WithPrefix\":      lambda data: get_label_distribution_wprefix(data),\n",
    "    \"WordDist\":        lambda data: get_word_distribution(data, common_vocab),\n",
    "    \"POSDist\":         lambda data: get_pos_distributions(data),\n",
    "    \"SpanLengthDist\":  lambda data: get_spanlength_distribution(data)\n",
    "}\n",
    "\n",
    "for method_name, dist_func in distribution_methods.items():\n",
    "    print(\"\\n\\n===========================================\")\n",
    "    print(f\"=== DISTRIBUTION METHOD: {method_name} ===\")\n",
    "    print(\"===========================================\")\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # A) TRAIN-TO-TRAIN\n",
    "    # -------------------------------------------\n",
    "    train_distributions = {}\n",
    "    for (name, data) in train_datasets:\n",
    "        train_distributions[name] = dist_func(data)\n",
    "\n",
    "    js_train_train, kl_train_train = compute_divergence_matrix(train_distributions)\n",
    "    print(\"\\n--- Train-to-Train ---\")\n",
    "    print(\"Jensen-Shannon Distance Matrix:\")\n",
    "    print(js_train_train)\n",
    "    print(\"\\nSymmetric KL Divergence Matrix:\")\n",
    "    print(kl_train_train)\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # B) DEV-TO-DEV\n",
    "    # -------------------------------------------\n",
    "    dev_distributions = {}\n",
    "    for (name, data) in dev_datasets:\n",
    "        dev_distributions[name] = dist_func(data)\n",
    "\n",
    "    js_dev_dev, kl_dev_dev = compute_divergence_matrix(dev_distributions)\n",
    "    print(\"\\n--- Dev-to-Dev ---\")\n",
    "    print(\"Jensen-Shannon Distance Matrix:\")\n",
    "    print(js_dev_dev)\n",
    "    print(\"\\nSymmetric KL Divergence Matrix:\")\n",
    "    print(kl_dev_dev)\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # C) TRAIN(deduplicated)-TO-DEV\n",
    "    # -------------------------------------------\n",
    "    n = len(train_datasets)\n",
    "    js_train_dev = np.zeros((n, n))\n",
    "    kl_train_dev = np.zeros((n, n))\n",
    "\n",
    "    for i in range(n):\n",
    "        train_name, train_data = train_datasets[i]\n",
    "        for j in range(n):\n",
    "            dev_name, dev_data = dev_datasets[j]\n",
    "\n",
    "            clean_data, _ = remove_leakage(train_data, dev_data)\n",
    "            P = dist_func(clean_data)\n",
    "            Q = dist_func(dev_data)\n",
    "\n",
    "            js_train_dev[i, j] = jensenshannon(P, Q)\n",
    "            kl_train_dev[i, j] = 0.5 * (entropy(P, Q) + entropy(Q, P))\n",
    "\n",
    "    print(\"\\n--- Train(deduplicated)-to-Dev ---\")\n",
    "    print(\"Jensen-Shannon Distance Matrix:\")\n",
    "    print(js_train_dev)\n",
    "    print(\"\\nSymmetric KL Divergence Matrix:\")\n",
    "    print(kl_train_dev)\n",
    "    print(\"===========================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {\n",
    "    \"IgnoringPrefix\": {\n",
    "        # updated raw label-count vectors (train splits)\n",
    "        \"LabelDistributions\": {\n",
    "            \"DNRTI\":     np.array([1777., 3835., 7937., 3219.]),\n",
    "            \"AttackNER\": np.array([ 901., 1886., 2608.,  914.]),\n",
    "            \"APTNER\":    np.array([4252.,  232., 6135.,  516.]),\n",
    "            \"CyNER\":     np.array([ 898., 1321.,  400.,   90.])\n",
    "        },\n",
    "        \"Train-to-Train_JS\": np.array([\n",
    "            [0.      , 0.07986706, 0.34266991, 0.36096433],\n",
    "            [0.07986706, 0.      , 0.34469139, 0.28697856],\n",
    "            [0.34266991, 0.34469139, 0.      , 0.44287795],\n",
    "            [0.36096433, 0.28697856, 0.44287795, 0.      ]\n",
    "        ]),\n",
    "        \"Train-to-Train_KL\": np.array([\n",
    "            [0.      , 0.02558817, 0.53522315, 0.55553726],\n",
    "            [0.02558817, 0.      , 0.56385104, 0.34459174],\n",
    "            [0.53522315, 0.56385104, 0.      , 1.00717131],\n",
    "            [0.55553726, 0.34459174, 1.00717131, 0.      ]\n",
    "        ]),\n",
    "        \"Dev-to-Dev_JS\": np.array([\n",
    "            [0.      , 0.09389453, 0.30797194, 0.31487711],\n",
    "            [0.09389453, 0.      , 0.34131368, 0.25890805],\n",
    "            [0.30797194, 0.34131368, 0.      , 0.29883119],\n",
    "            [0.31487711, 0.25890805, 0.29883119, 0.      ]\n",
    "        ]),\n",
    "        \"Dev-to-Dev_KL\": np.array([\n",
    "            [0.      , 0.03543253, 0.42711739, 0.43428761],\n",
    "            [0.03543253, 0.      , 0.51776628, 0.29600349],\n",
    "            [0.42711739, 0.51776628, 0.      , 0.3880076 ],\n",
    "            [0.43428761, 0.29600349, 0.3880076 , 0.      ]\n",
    "        ]),\n",
    "        \"TrainDedup-to-Dev_JS\": np.array([\n",
    "            [0.05354488, 0.10159504, 0.32967353, 0.33408855],\n",
    "            [0.06914041, 0.04223393, 0.31592283, 0.26375104],\n",
    "            [0.32756907, 0.36976507, 0.10593437, 0.3692673 ],\n",
    "            [0.34529086, 0.27108238, 0.3819674 , 0.09414017]\n",
    "        ]),\n",
    "        \"TrainDedup-to-Dev_KL\": np.array([\n",
    "            [0.01153706, 0.04145772, 0.49296368, 0.49289361],\n",
    "            [0.01917479, 0.00714002, 0.43904301, 0.30049962],\n",
    "            [0.48942613, 0.6540482 , 0.04741653, 0.68136262],\n",
    "            [0.50537305, 0.30932209, 0.6512033 , 0.03565283]\n",
    "        ])\n",
    "    },\n",
    "    \"SpanBased\": {\n",
    "        \"Train-to-Train_JS\": np.array([\n",
    "            [0.      , 0.13617656, 0.31196765, 0.3691458 ],\n",
    "            [0.13617656, 0.      , 0.28220034, 0.27089175],\n",
    "            [0.31196765, 0.28220034, 0.      , 0.42346006],\n",
    "            [0.3691458 , 0.27089175, 0.42346006, 0.      ]\n",
    "        ]),\n",
    "        \"Train-to-Train_KL\": np.array([\n",
    "            [0.      , 0.07614166, 0.43688426, 0.58493091],\n",
    "            [0.07614166, 0.      , 0.37981549, 0.30479624],\n",
    "            [0.43688426, 0.37981549, 0.      , 0.88910906],\n",
    "            [0.58493091, 0.30479624, 0.88910906, 0.      ]\n",
    "        ]),\n",
    "        \"Dev-to-Dev_JS\": np.array([\n",
    "            [0.      , 0.1177363 , 0.29543219, 0.36468549],\n",
    "            [0.1177363 , 0.      , 0.25518505, 0.2925345 ],\n",
    "            [0.29543219, 0.25518505, 0.      , 0.27911849],\n",
    "            [0.36468549, 0.2925345 , 0.27911849, 0.      ]\n",
    "        ]),\n",
    "        \"Dev-to-Dev_KL\": np.array([\n",
    "            [0.      , 0.05676031, 0.38686036, 0.5824755 ],\n",
    "            [0.05676031, 0.      , 0.27376642, 0.35868638],\n",
    "            [0.38686036, 0.27376642, 0.      , 0.3299068 ],\n",
    "            [0.5824755 , 0.35868638, 0.3299068 , 0.      ]\n",
    "        ]),\n",
    "        \"TrainDedup-to-Dev_JS\": np.array([\n",
    "            [0.04685495, 0.12868213, 0.3132396 , 0.38174753],\n",
    "            [0.1224758 , 0.02629667, 0.23711523, 0.27958455],\n",
    "            [0.30247096, 0.29710179, 0.13268362, 0.37969063],\n",
    "            [0.35462154, 0.2792152 , 0.33235349, 0.08494395]\n",
    "        ]),\n",
    "        \"TrainDedup-to-Dev_KL\": np.array([\n",
    "            [0.00881604, 0.06788617, 0.43802178, 0.64392931],\n",
    "            [0.06149291, 0.00276825, 0.23520263, 0.32598835],\n",
    "            [0.41167295, 0.42170814, 0.07445009, 0.68091531],\n",
    "            [0.53662785, 0.32455711, 0.4794519 , 0.02894749]\n",
    "        ])\n",
    "    },\n",
    "    \"WithPrefix\": {\n",
    "        \"LabelDistributions\": {\n",
    "            \"DNRTI\":     np.array([1221., 5529., 2449., 1896.,  556., 2408., 1386., 1323.]),\n",
    "            \"AttackNER\": np.array([ 548., 1390.,  822.,  197.,  353., 1218., 1064.,  717.]),\n",
    "            \"APTNER\":    np.array([3109., 4742.,  205.,  483., 1143., 1393.,   27.,   33.]),\n",
    "            \"CyNER\":     np.array([ 703.,  284.,  837.,   48.,  195.,  116.,  484.,   42.])\n",
    "        },\n",
    "        \"Train-to-Train_JS\": np.array([\n",
    "            [0.      , 0.17526216, 0.35662361, 0.36246624],\n",
    "            [0.17526216, 0.      , 0.40123206, 0.31647466],\n",
    "            [0.35662361, 0.40123206, 0.      , 0.4511981 ],\n",
    "            [0.36246624, 0.31647466, 0.4511981 , 0.      ]\n",
    "        ]),\n",
    "        \"Train-to-Train_KL\": np.array([\n",
    "            [0.      , 0.1271366 , 0.62326889, 0.56148289],\n",
    "            [0.1271366 , 0.      , 0.87478338, 0.43151897],\n",
    "            [0.62326889, 0.87478338, 0.      , 1.08575745],\n",
    "            [0.56148289, 0.43151897, 1.08575745, 0.      ]\n",
    "        ]),\n",
    "        \"Dev-to-Dev_JS\": np.array([\n",
    "            [0.      , 0.20736572, 0.33133543, 0.34510501],\n",
    "            [0.20736572, 0.      , 0.41202332, 0.34592461],\n",
    "            [0.33133543, 0.41202332, 0.      , 0.33482917],\n",
    "            [0.34510501, 0.34592461, 0.33482917, 0.      ]\n",
    "        ]),\n",
    "        \"Dev-to-Dev_KL\": np.array([\n",
    "            [0.      , 0.17871046, 0.56430059, 0.5244984 ],\n",
    "            [0.17871046, 0.      , 1.06295504, 0.55254076],\n",
    "            [0.56430059, 1.06295504, 0.      , 0.61245039],\n",
    "            [0.5244984 , 0.55254076, 0.61245039, 0.      ]\n",
    "        ]),\n",
    "        \"TrainDedup-to-Dev_JS\": np.array([\n",
    "            [0.05801686, 0.19746368, 0.35325099, 0.35786743],\n",
    "            [0.18612717, 0.06471221, 0.38242566, 0.32570309],\n",
    "            [0.33809908, 0.42432434, 0.13128673, 0.40985553],\n",
    "            [0.34774526, 0.3154299 , 0.40147882, 0.15615116]\n",
    "        ]),\n",
    "        \"TrainDedup-to-Dev_KL\": np.array([\n",
    "            [0.01354271, 0.16183411, 0.65248715, 0.57155545],\n",
    "            [0.14337967, 0.01679723, 0.88743652, 0.47704894],\n",
    "            [0.55443361, 1.01431812, 0.0723788 , 0.85200446],\n",
    "            [0.51406981, 0.43269824, 0.91071757, 0.10223415]\n",
    "        ])\n",
    "    },\n",
    "    \"WordDist\": {\n",
    "        \"Train-to-Train_JS\": np.array([\n",
    "            [0.      , 0.35760725, 0.13831401, 0.40274952],\n",
    "            [0.35760725, 0.      , 0.3357986 , 0.37419193],\n",
    "            [0.13831401, 0.3357986 , 0.      , 0.36833671],\n",
    "            [0.40274952, 0.37419193, 0.36833671, 0.      ]\n",
    "        ]),\n",
    "        \"Train-to-Train_KL\": np.array([\n",
    "            [0.      , 2.86754119, 0.32412469, 3.30223229],\n",
    "            [2.86754119, 0.      , 2.46597506, 3.15551478],\n",
    "            [0.32412469, 2.46597506, 0.      , 2.61344688],\n",
    "            [3.30223229, 3.15551478, 2.61344688, 0.      ]\n",
    "        ]),\n",
    "        \"Dev-to-Dev_JS\": np.array([\n",
    "            [0.      , 0.43331349, 0.37307906, 0.4707207 ],\n",
    "            [0.43331349, 0.      , 0.40500397, 0.46921746],\n",
    "            [0.37307906, 0.40500397, 0.      , 0.41378029],\n",
    "            [0.4707207 , 0.46921746, 0.41378029, 0.      ]\n",
    "        ]),\n",
    "        \"Dev-to-Dev_KL\": np.array([\n",
    "            [0.      , 5.28401586, 3.61465548, 5.94851187],\n",
    "            [5.28401586, 0.      , 4.36613438, 6.16659823],\n",
    "            [3.61465548, 4.36613438, 0.      , 4.47241725],\n",
    "            [5.94851187, 6.16659823, 4.47241725, 0.      ]\n",
    "        ]),\n",
    "        \"TrainDedup-to-Dev_JS\": np.array([\n",
    "            [0.31258369, 0.41659888, 0.32964615, 0.45355186],\n",
    "            [0.38949561, 0.34087477, 0.34555944, 0.42380335],\n",
    "            [0.31578788, 0.40334397, 0.29630735, 0.42663962],\n",
    "            [0.43080244, 0.43586832, 0.36634973, 0.30713639]\n",
    "        ]),\n",
    "        \"TrainDedup-to-Dev_KL\": np.array([\n",
    "            [2.37480618, 4.20415643, 2.41297445, 4.62927543],\n",
    "            [3.90758582, 2.89750329, 2.97269921, 4.29816212],\n",
    "            [2.29500266, 3.78459083, 1.91280747, 3.87598426],\n",
    "            [4.6078584 , 4.854387  , 3.36104378, 2.31597525]\n",
    "        ])\n",
    "    },\n",
    "    \"POSDist\": {\n",
    "        \"Train-to-Train_JS\": np.array([\n",
    "            [0.      , 0.04199404, 0.02111907, 0.05641533],\n",
    "            [0.04199404, 0.      , 0.04111081, 0.0438426 ],\n",
    "            [0.02111907, 0.04111081, 0.      , 0.04155997],\n",
    "            [0.05641533, 0.0438426 , 0.04155997, 0.      ]\n",
    "        ]),\n",
    "        \"Train-to-Train_KL\": np.array([\n",
    "            [0.      , 0.00728999, 0.00180301, 0.01295374],\n",
    "            [0.00728999, 0.      , 0.0069864 , 0.00797699],\n",
    "            [0.00180301, 0.0069864 , 0.      , 0.00692823],\n",
    "            [0.01295374, 0.00797699, 0.00692823, 0.      ]\n",
    "        ]),\n",
    "        \"Dev-to-Dev_JS\": np.array([\n",
    "            [0.      , 0.04413039, 0.04106488, 0.0607105 ],\n",
    "            [0.04413039, 0.      , 0.04469234, 0.05426046],\n",
    "            [0.04106488, 0.04469234, 0.      , 0.0312925 ],\n",
    "            [0.0607105 , 0.05426046, 0.0312925 , 0.      ]\n",
    "        ]),\n",
    "        \"Dev-to-Dev_KL\": np.array([\n",
    "            [0.      , 0.00919047, 0.00687539, 0.01489396],\n",
    "            [0.00919047, 0.      , 0.00862162, 0.0125142 ],\n",
    "            [0.00687539, 0.00862162, 0.      , 0.00392231],\n",
    "            [0.01489396, 0.0125142 , 0.00392231, 0.      ]\n",
    "        ]),\n",
    "        \"TrainDedup-to-Dev_JS\": np.array([\n",
    "            [0.01770765, 0.04133073, 0.04087191, 0.06165252],\n",
    "            [0.04329741, 0.01661484, 0.04359336, 0.0485763 ],\n",
    "            [0.02402037, 0.04209156, 0.02560897, 0.04625693],\n",
    "            [0.05716238, 0.0485405 , 0.02797081, 0.02464628]\n",
    "        ]),\n",
    "        \"TrainDedup-to-Dev_KL\": np.array([\n",
    "            [0.0012581 , 0.00746096, 0.00685219, 0.01541122],\n",
    "            [0.00813797, 0.00110822, 0.0078107 , 0.00968479],\n",
    "            [0.00231872, 0.00775144, 0.00263516, 0.00857853],\n",
    "            [0.01322847, 0.01027207, 0.00313798, 0.00243258]\n",
    "        ])\n",
    "    },\n",
    "    \"SpanLengthDist\": {\n",
    "        \"Train-to-Train_JS\": np.array([\n",
    "            [0.      , 0.22936515, 0.03539619, 0.05009848],\n",
    "            [0.22936515, 0.      , 0.24052952, 0.19019921],\n",
    "            [0.03539619, 0.24052952, 0.      , 0.06866643],\n",
    "            [0.05009848, 0.19019921, 0.06866643, 0.      ]\n",
    "        ]),\n",
    "        \"Train-to-Train_KL\": np.array([\n",
    "            [0.      , 0.26270405, 0.00509496, 0.01044159],\n",
    "            [0.26270405, 0.      , 0.31248292, 0.16354544],\n",
    "            [0.00509496, 0.31248292, 0.      , 0.01984823],\n",
    "            [0.01044159, 0.16354544, 0.01984823, 0.      ]\n",
    "        ]),\n",
    "        \"Dev-to-Dev_JS\": np.array([\n",
    "            [0.      , 0.28793086, 0.12249062, 0.13124202],\n",
    "            [0.28793086, 0.      , 0.34799242, 0.21926447],\n",
    "            [0.12249062, 0.34799242, 0.      , 0.15046813],\n",
    "            [0.13124202, 0.21926447, 0.15046813, 0.      ]\n",
    "        ]),\n",
    "        \"Dev-to-Dev_KL\": np.array([\n",
    "            [0.      , 0.88962542, 0.11451862, 0.46279672],\n",
    "            [0.88962542, 0.      , 0.8020282 , 0.19822299],\n",
    "            [0.11451862, 0.8020282 , 0.      , 0.2797514 ],\n",
    "            [0.46279672, 0.19822299, 0.2797514 , 0.      ]\n",
    "        ]),\n",
    "        \"TrainDedup-to-Dev_JS\": np.array([\n",
    "            [0.0754726 , 0.25898799, 0.14830318, 0.10556701],\n",
    "            [0.26480789, 0.07800091, 0.31936959, 0.18108133],\n",
    "            [0.08458899, 0.27316095, 0.13981647, 0.10533993],\n",
    "            [0.10519283, 0.22426471, 0.17647083, 0.08704456]\n",
    "        ]),\n",
    "        \"TrainDedup-to-Dev_KL\": np.array([\n",
    "            [0.16980204, 0.3483377 , 0.11222577, 0.05416863],\n",
    "            [1.16205431, 0.02458686, 0.85702472, 0.13339517],\n",
    "            [0.20248189, 0.43260172, 0.13043904, 0.06378875],\n",
    "            [0.30332294, 0.23889539, 0.21647758, 0.0321578 ]\n",
    "        ])\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jensen–Shannon correlations written to js_correlation.xlsx\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "#span-F1 performance matrix  (update if values change)\n",
    "perf_matrix = np.array([\n",
    "    [0.41, 0.16, 0.19, 0.07],\n",
    "    [0.09, 0.23, 0.01, 0.02],\n",
    "    [0.31, 0.16, 0.41, 0.18],\n",
    "    [0.05, 0.04, 0.06, 0.40]\n",
    "])\n",
    "\n",
    "mask      = ~np.eye(len(dataset_names), dtype=bool)   # off-diagonal mask\n",
    "perf_vec  = perf_matrix[mask]                         # 12-element vector\n",
    "\n",
    "#correlate ONLY Jensen-Shannon matrices with performance\n",
    "rows = []\n",
    "for method, matdict in all_results.items():           # all_results already in memory\n",
    "    for label, mat in matdict.items():\n",
    "        if \"JS\" in label and isinstance(mat, np.ndarray) and mat.shape == (4, 4):\n",
    "            div_vec      = mat[mask]\n",
    "            r, p         = pearsonr(perf_vec, div_vec)\n",
    "            rows.append({\"Method\": method,\n",
    "                         \"Matrix\": label,\n",
    "                         \"Pearson_r\": r,\n",
    "                         \"p_value\": p})\n",
    "\n",
    "corr_df = pd.DataFrame(rows).sort_values(\"Pearson_r\")\n",
    "\n",
    "#excel\n",
    "out_file = \"js_correlation.xlsx\"\n",
    "corr_df.to_excel(out_file, index=False, sheet_name=\"JS_Correlations\")\n",
    "print(f\"Jensen–Shannon correlations written to {out_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.9 ('nlpproject_py312')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0af6a56369cba92bb59bd2db7ebca9b2b7c118936f54fea9e113554e0538962b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
