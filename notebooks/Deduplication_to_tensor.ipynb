{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-06 14:20:05.868\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnlp_cyber_ner.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: C:\\Users\\johan\\OneDrive\\Skrivebord\\Explorer\\ITU\\4_semester\\NLP_and_deep_learning\\Exam_project\\repo_new_push_github\\NLP-Cyber-NER\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")  # points to the root of the repo\n",
    "\n",
    "from nlp_cyber_ner.dataset import (\n",
    "    read_iob2_file,\n",
    "    remove_leakage,\n",
    "    prepare_cross_dataset,\n",
    "    Preprocess\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = {\n",
    "    \"aptner\": (\n",
    "        read_iob2_file,\n",
    "        \"APTNERtrain.unified\",\n",
    "        \"APTNERdev.unified\",\n",
    "        \"APTNERtest.unified\",\n",
    "        \"APTNER\",\n",
    "        {\"word_index\": 0, \"tag_index\": 1},\n",
    "        \"processed\"\n",
    "    ),\n",
    "    \"dnrti\": (\n",
    "        read_iob2_file,\n",
    "        \"train.unified\",\n",
    "        \"valid.unified\",\n",
    "        \"test.unified\",\n",
    "        \"dnrti\",\n",
    "        {\"word_index\": 0, \"tag_index\": 1},\n",
    "        \"processed\"\n",
    "    ),\n",
    "    \"attackner\": (\n",
    "        read_iob2_file,\n",
    "        \"train.unified\",\n",
    "        \"dev.unified\",\n",
    "        \"test.unified\",\n",
    "        \"attackner\",\n",
    "        {\"word_index\": 0, \"tag_index\": 1},\n",
    "        \"processed\"\n",
    "    ),\n",
    "    \"cyner\": (\n",
    "        read_iob2_file,\n",
    "        \"train.unified\",\n",
    "        \"valid.unified\",\n",
    "        \"test.unified\",\n",
    "        \"cyner\",\n",
    "        {\"word_index\": 0, \"tag_index\": 1},\n",
    "        \"processed\" \n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training on aptner | Testing on aptner\n",
      "Removed 105 leaked sentences from training data.\n",
      "Removed 28 leaked sentences from training data.\n",
      "Removed 105 train and 28 dev overlapping sentences.\n",
      "Stored 6640 training samples for aptner → aptner\n",
      "\n",
      " Training on aptner | Testing on dnrti\n",
      "Removed 263 leaked sentences from training data.\n",
      "Removed 1 leaked sentences from training data.\n",
      "Removed 263 train and 1 dev overlapping sentences.\n",
      "Stored 6482 training samples for aptner → dnrti\n",
      "\n",
      " Training on aptner | Testing on attackner\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 0 train and 0 dev overlapping sentences.\n",
      "Stored 6745 training samples for aptner → attackner\n",
      "\n",
      " Training on aptner | Testing on cyner\n",
      "Removed 20 leaked sentences from training data.\n",
      "Removed 1 leaked sentences from training data.\n",
      "Removed 20 train and 1 dev overlapping sentences.\n",
      "Stored 6725 training samples for aptner → cyner\n",
      "\n",
      " Training on dnrti | Testing on aptner\n",
      "Removed 127 leaked sentences from training data.\n",
      "Removed 1 leaked sentences from training data.\n",
      "Removed 127 train and 1 dev overlapping sentences.\n",
      "Stored 5124 training samples for dnrti → aptner\n",
      "\n",
      " Training on dnrti | Testing on dnrti\n",
      "Removed 626 leaked sentences from training data.\n",
      "Removed 66 leaked sentences from training data.\n",
      "Removed 626 train and 66 dev overlapping sentences.\n",
      "Stored 4625 training samples for dnrti → dnrti\n",
      "\n",
      " Training on dnrti | Testing on attackner\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 0 train and 0 dev overlapping sentences.\n",
      "Stored 5251 training samples for dnrti → attackner\n",
      "\n",
      " Training on dnrti | Testing on cyner\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 0 train and 0 dev overlapping sentences.\n",
      "Stored 5251 training samples for dnrti → cyner\n",
      "\n",
      " Training on attackner | Testing on aptner\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 0 train and 0 dev overlapping sentences.\n",
      "Stored 2481 training samples for attackner → aptner\n",
      "\n",
      " Training on attackner | Testing on dnrti\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 0 train and 0 dev overlapping sentences.\n",
      "Stored 2481 training samples for attackner → dnrti\n",
      "\n",
      " Training on attackner | Testing on attackner\n",
      "Removed 109 leaked sentences from training data.\n",
      "Removed 21 leaked sentences from training data.\n",
      "Removed 109 train and 21 dev overlapping sentences.\n",
      "Stored 2372 training samples for attackner → attackner\n",
      "\n",
      " Training on attackner | Testing on cyner\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 0 train and 0 dev overlapping sentences.\n",
      "Stored 2481 training samples for attackner → cyner\n",
      "\n",
      " Training on cyner | Testing on aptner\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 0 train and 0 dev overlapping sentences.\n",
      "Stored 2811 training samples for cyner → aptner\n",
      "\n",
      " Training on cyner | Testing on dnrti\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 0 train and 0 dev overlapping sentences.\n",
      "Stored 2811 training samples for cyner → dnrti\n",
      "\n",
      " Training on cyner | Testing on attackner\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 0 leaked sentences from training data.\n",
      "Removed 0 train and 0 dev overlapping sentences.\n",
      "Stored 2811 training samples for cyner → attackner\n",
      "\n",
      " Training on cyner | Testing on cyner\n",
      "Removed 17 leaked sentences from training data.\n",
      "Removed 5 leaked sentences from training data.\n",
      "Removed 17 train and 5 dev overlapping sentences.\n",
      "Stored 2794 training samples for cyner → cyner\n"
     ]
    }
   ],
   "source": [
    "all_results = {}\n",
    "\n",
    "for train_name, (train_reader, train_file, dev_file, _, train_folder, train_kwargs, train_subdir) in DATASETS.items():\n",
    "    for test_name, (test_reader, _, _, test_file, test_folder, test_kwargs, test_subdir) in DATASETS.items():\n",
    "        print(f\"\\n Training on {train_name} | Testing on {test_name}\")\n",
    "\n",
    "    \n",
    "        train_path = os.path.join(\"..\", \"data\", train_subdir, train_folder, train_file)\n",
    "        dev_path   = os.path.join(\"..\", \"data\", train_subdir, train_folder, dev_file)\n",
    "        test_path  = os.path.join(\"..\", \"data\", test_subdir, test_folder, test_file)\n",
    "\n",
    "        if not os.path.exists(train_path) or not os.path.exists(test_path):\n",
    "            print(\" Missing data files, skipping.\")\n",
    "            continue\n",
    "        if not os.path.exists(dev_path):\n",
    "            print(f\" Dev file missing for {train_name}, using empty dev set.\")\n",
    "            dev_path = None\n",
    "\n",
    "        # Run all the preparation \n",
    "        transformer, train_X, train_y, dev_X, dev_y, test_X, test_y, idx2word, idx2label = prepare_cross_dataset(\n",
    "            train_reader=train_reader,\n",
    "            dev_reader=train_reader,  \n",
    "            test_reader=test_reader,\n",
    "            train_path=train_path,\n",
    "            dev_path=dev_path,\n",
    "            test_path=test_path,\n",
    "            remove_leakage_from_test=True,\n",
    "            dev_split_ratio=None,\n",
    "            reader_kwargs=train_kwargs\n",
    "        )\n",
    "\n",
    "        # Save everything to dictionary\n",
    "        all_results[(train_name, test_name)] = {\n",
    "            \"transformer\": transformer,\n",
    "            \"train_X\": train_X,\n",
    "            \"train_y\": train_y,\n",
    "            \"dev_X\": dev_X,\n",
    "            \"dev_y\": dev_y,\n",
    "            \"test_X\": test_X,\n",
    "            \"test_y\": test_y,\n",
    "            \"idx2word\": idx2word,\n",
    "            \"idx2label\": idx2label,\n",
    "        }\n",
    "\n",
    "        print(f\"Stored {train_X.shape[0]} training samples for {train_name} → {test_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X shape: torch.Size([6482, 5862])\n",
      "train_y shape: torch.Size([6482, 5862])\n",
      "dev_X shape: torch.Size([1751, 5862])\n",
      "dev_y shape: torch.Size([1751, 5862])\n",
      "test_X shape: torch.Size([664, 5862])\n",
      "test_y shape: torch.Size([664, 5862])\n"
     ]
    }
   ],
   "source": [
    "entry = all_results[(\"aptner\", \"dnrti\")]\n",
    "\n",
    "print(\"train_X shape:\", entry[\"train_X\"].shape)\n",
    "print(\"train_y shape:\", entry[\"train_y\"].shape)\n",
    "print(\"dev_X shape:\", entry[\"dev_X\"].shape if entry[\"dev_X\"] is not None else \"None\")\n",
    "print(\"dev_y shape:\", entry[\"dev_y\"].shape if entry[\"dev_y\"] is not None else \"None\")\n",
    "print(\"test_X shape:\", entry[\"test_X\"].shape)\n",
    "print(\"test_y shape:\", entry[\"test_y\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_conda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
