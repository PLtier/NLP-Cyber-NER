{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project NLP and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Project proposal presentation\n",
    "\n",
    "In the presentation, you have 5 minutes to present your research proposal. During the presentation, you should explain:\n",
    "\n",
    "* What is the topic of your project, what is the current state of this topic/task/setup\n",
    "* What is the new part of your project\n",
    "* What is the research question of your project\n",
    "\n",
    "We have proposed a number of topics in the slides which can be found on LearnIt, you can either pick one of these or come up with your own. If you pick your own, we suggest to get a pre-approval with Rob van der Goot.\n",
    "\n",
    "**Deadline for uploading slides: day before the presentation (23:59)**  (pdf only, they will be put into one long pdf for a smooth presentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Baseline\n",
    "To get your project started, you start with implementing a baseline model. Ideally, this is going to be the main baseline that you are going to compare to in your paper. Note that this baseline should be more advanced than just predicting the majority class (O).\n",
    "\n",
    "We will use EWT portion of the [Universal NER project](http://www.universalner.org/), which we provide with this notebook for convenience. You can use the train data (`en_ewt-ud-train.iob2`) and dev data(`en_ewt-ud-dev.iob2`) to build your baseline, then upload your prediction on the test data (`en_ewt-ud-test.iob2`).\n",
    "\n",
    "It is important to upload your predictions in same format as the training and dev files, so that the `span_f1.py` script can be used.\n",
    "\n",
    "Note that you do not have to implement your baseline from scratch, you can use for example the code from the RNN or BERT assignments as a starting point.\n",
    "\n",
    "**Deadline: 20-03 on LearnIt (14:00)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desktop2.hpc.itu.dk\n"
     ]
    }
   ],
   "source": [
    "!uname --nodename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting jsonlines\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/itu/easybuild/software/Python-bundle-PyPI/2023.06-GCCcore-12.3.0/lib/python3.11/site-packages (from jsonlines) (23.1.0)\n",
      "Installing collected packages: jsonlines\n",
      "Successfully installed jsonlines-4.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:           30Gi       841Mi       989Mi       101Mi        29Gi        29Gi\n",
      "Swap:          15Gi       7.0Mi        15Gi\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-03 18:50:09.466\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnlp_cyber_ner.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: /home/macja/repositories/NLP-Cyber-NER\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from nlp_cyber_ner.dataset import read_cyner, read_aptner, read_attackner, read_dnrti\n",
    "from nlp_cyber_ner.dataset import unify_labels_aptner\n",
    "from nlp_cyber_ner.dataset import clean_aptner, clean_dnrti\n",
    "from nlp_cyber_ner.dataset import transform_dataset\n",
    "from nlp_cyber_ner.config import PROCESSED_DATA_DIR, RAW_DATA_DIR, INTERIM_DATA_DIR\n",
    "from nlp_cyber_ner.dataset import Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cyner Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyner_path = RAW_DATA_DIR / \"cyner\"\n",
    "cyner_train_path = cyner_path / \"train.txt\"\n",
    "cyner_dev_path = cyner_path / \"valid.txt\"\n",
    "cyner_test_path = cyner_path / \"test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_cyber_ner.dataset import unify_labels_cyner\n",
    "unify_labels_cyner(path=cyner_train_path)\n",
    "unify_labels_cyner(path=cyner_dev_path)\n",
    "unify_labels_cyner(path=cyner_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_cyber_ner.dataset import read_iob2_file\n",
    "\n",
    "\n",
    "cyner_path = PROCESSED_DATA_DIR / \"cyner\"\n",
    "cyner_train_path = cyner_path / \"train.unified\"\n",
    "cyner_dev_path = cyner_path / \"valid.unified\"\n",
    "cyner_test_path = cyner_path / \"test.unified\"\n",
    "\n",
    "cyner_train_data = read_iob2_file(cyner_train_path)\n",
    "cyner_dev_data = read_iob2_file(cyner_dev_path)\n",
    "cyner_test_data = read_iob2_file(cyner_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyner_pack = transform_dataset(\n",
    "    cyner_train_data, cyner_dev_data, cyner_test_data,\n",
    "    True\n",
    ")\n",
    "cyner_train_X, cyner_train_y, cyner_dev_X, cyner_dev_y, cyner_test_X, cyner_test_y, cyner_idx2word, cyner_idx2label, cyner_max_len = cyner_pack\n",
    "\n",
    "# cyner_max_len = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<PAD>',\n",
       " 'B-Malware',\n",
       " 'I-Malware',\n",
       " 'O',\n",
       " 'B-System',\n",
       " 'I-System',\n",
       " 'B-Organization',\n",
       " 'B-O',\n",
       " 'I-Organization',\n",
       " 'I-O',\n",
       " 'B-Vulnerability',\n",
       " 'I-Vulnerability']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cyner_idx2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aptner Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'C:\\\\Users\\\\Maciej\\\\coding-projects\\\\NLP-Cyber-NER\\\\data\\\\raw\\\\APTNer\\\\APTNERtest.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m clean_aptner(aptner_train_path)\n\u001b[32m      6\u001b[39m clean_aptner(aptner_dev_path)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mclean_aptner\u001b[49m\u001b[43m(\u001b[49m\u001b[43maptner_test_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# move to interim folder\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\coding-projects\\NLP-Cyber-NER\\nlp_cyber_ner\\dataset.py:88\u001b[39m, in \u001b[36mclean_aptner\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclean_aptner\u001b[39m(path: Path) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     84\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[33;03m    If there is only one token in the sentence, it is assigned the label O. The function saves cleaned data.\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m         \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f,\n\u001b[32m     89\u001b[39m         \u001b[38;5;28mopen\u001b[39m(path.with_suffix(\u001b[33m\"\u001b[39m\u001b[33m.cleaned\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f_out,\n\u001b[32m     90\u001b[39m     ):\n\u001b[32m     91\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[32m     92\u001b[39m             line = line.strip()\n",
      "\u001b[31mOSError\u001b[39m: [Errno 22] Invalid argument: 'C:\\\\Users\\\\Maciej\\\\coding-projects\\\\NLP-Cyber-NER\\\\data\\\\raw\\\\APTNer\\\\APTNERtest.txt'"
     ]
    }
   ],
   "source": [
    "aptner_path = RAW_DATA_DIR / \"APTNer\"\n",
    "aptner_train_path= aptner_path / \"APTNERtrain.txt\"\n",
    "aptner_dev_path= aptner_path / \"APTNERdev.txt\"\n",
    "aptner_test_path= aptner_path / \"APTNERtest.txt\"\n",
    "clean_aptner(aptner_train_path)\n",
    "clean_aptner(aptner_dev_path)\n",
    "clean_aptner(aptner_test_path)\n",
    "# move to interim folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aptner_path = INTERIM_DATA_DIR / \"APTNer\"\n",
    "aptner_train_path= aptner_path / \"APTNERtrain.cleaned\"\n",
    "aptner_dev_path= aptner_path / \"APTNERdev.cleaned\"\n",
    "aptner_test_path= aptner_path / \"APTNERtest.cleaned\"\n",
    "unify_labels_aptner(aptner_train_path)\n",
    "unify_labels_aptner(aptner_dev_path)\n",
    "unify_labels_aptner(aptner_test_path)\n",
    "# move to processed folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aptner_path = PROCESSED_DATA_DIR / \"APTNer\"\n",
    "aptner_train_path= aptner_path / \"APTNERtrain.unified\"\n",
    "aptner_dev_path= aptner_path / \"APTNERdev.unified\"\n",
    "aptner_test_path= aptner_path / \"APTNERtest.unified\"\n",
    "aptner_train_data = read_aptner(aptner_train_path)\n",
    "aptner_dev_data = read_aptner(aptner_dev_path)\n",
    "aptner_test_data = read_aptner(aptner_test_path)\n",
    "end_labels = {'B-Organization', 'O', 'I-Malware', 'B-System', 'I-Vulnerability', 'I-Organization', 'I-System', 'B-Vulnerability', 'B-Malware'}\n",
    "A = set(tag for _, tags in aptner_train_data for tag in tags)\n",
    "B = set(tag for _, tags in aptner_dev_data for tag in tags)\n",
    "C = set(tag for _, tags in aptner_test_data for tag in tags)\n",
    "assert A == B == C == end_labels, \"The labels in the train, dev and test sets are not the same.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "aptner_pack = transform_dataset(\n",
    "    aptner_train_data, aptner_dev_data, aptner_test_data, True\n",
    ")\n",
    "aptner_train_X, aptner_train_y, aptner_dev_X, aptner_dev_y, aptner_test_X, aptner_test_y, aptner_idx2word, aptner_idx2label, aptner_max_len = aptner_pack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AttackNer loacders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_cyber_ner.dataset import unify_labels_attackner, read_iob2_file\n",
    "\n",
    "attackner_path = RAW_DATA_DIR / \"attackner\"\n",
    "attackner_train_path  = attackner_path / \"train.json\"\n",
    "attackner_dev_path= attackner_path / \"dev.json\"\n",
    "attackner_test_path= attackner_path / \"test.json\"\n",
    "\n",
    "\n",
    "unify_labels_attackner(attackner_test_path)\n",
    "unify_labels_attackner(attackner_train_path)\n",
    "unify_labels_attackner(attackner_dev_path)\n",
    "# actually, a little cleaning has been done. For now, unification and cleaning are in the same step\n",
    "# move to processed folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "attackner_path = PROCESSED_DATA_DIR / \"attackner\"\n",
    "attackner_train_path  = attackner_path / \"train.unified\"\n",
    "attackner_dev_path= attackner_path / \"dev.unified\"\n",
    "attackner_test_path= attackner_path / \"test.unified\"\n",
    "attackner_train_data = read_iob2_file(attackner_train_path, word_index=0, tag_index=1)\n",
    "attackner_dev_data = read_iob2_file(attackner_dev_path, word_index=0, tag_index=1)\n",
    "attackner_test_data = read_iob2_file(attackner_test_path, word_index=0, tag_index=1)\n",
    "end_labels = {'B-Organization', 'O', 'I-Malware', 'B-System', 'I-Vulnerability', 'I-Organization', 'I-System', 'B-Vulnerability', 'B-Malware'}\n",
    "A = set(tag for _, tags in attackner_train_data for tag in tags)\n",
    "B = set(tag for _, tags in attackner_dev_data for tag in tags)\n",
    "C = set(tag for _, tags in attackner_test_data for tag in tags)\n",
    "assert A == B == C == end_labels, \"The labels in the train_data, dev_data and test_data sets are not the same.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<PAD>',\n",
       " 'O',\n",
       " 'B-Organization',\n",
       " 'B-System',\n",
       " 'I-System',\n",
       " 'B-Vulnerability',\n",
       " 'I-Vulnerability',\n",
       " 'B-Malware',\n",
       " 'I-Organization',\n",
       " 'I-Malware']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attackner_pack = transform_dataset(\n",
    "    attackner_train_data, attackner_dev_data, attackner_test_data, True\n",
    ")\n",
    "attackner_train_X, attackner_train_y, attackner_dev_X, attackner_dev_y, attackner_test_X, attackner_test_y, attackner_idx2word, attackner_idx2label, attackner_max_len = attackner_pack\n",
    "\n",
    "attackner_idx2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DN-RTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-31 23:27:08.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnlp_cyber_ner.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: D:\\Projekty Programistyczne\\NLP-Cyber-NER\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projekty Programistyczne\\NLP-Cyber-NER\\.venv\\Lib\\site-packages\\importlib_metadata\\__init__.py:795\u001b[39m, in \u001b[36mFastPath.mtime\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    794\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m suppress(\u001b[38;5;167;01mOSError\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m795\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m.st_mtime\n\u001b[32m    796\u001b[39m \u001b[38;5;28mself\u001b[39m.lookup.cache_clear()\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 2] The system cannot find the file specified: 'C:\\\\Users\\\\PLtier\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\python312.zip'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnlp_cyber_ner\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m read_iob2_file, clean_dnrti, unify_labels_dnrti\n\u001b[32m      2\u001b[39m dnrti_path = RAW_DATA_DIR / \u001b[33m\"\u001b[39m\u001b[33mDNRTI\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m dnrti_train_path = dnrti_path / \u001b[33m\"\u001b[39m\u001b[33mtrain.txt\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Projekty Programistyczne\\NLP-Cyber-NER\\nlp_cyber_ner\\dataset.py:6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjsonlines\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrandom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m f\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      8\u001b[39m aptner_labels = \u001b[38;5;28mset\u001b[39m(\n\u001b[32m      9\u001b[39m     [\n\u001b[32m     10\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mB-ACT\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     79\u001b[39m     ]\n\u001b[32m     80\u001b[39m )\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclean_aptner\u001b[39m(path: Path) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projekty Programistyczne\\NLP-Cyber-NER\\.venv\\Lib\\site-packages\\torch\\__init__.py:2108\u001b[39m\n\u001b[32m   2101\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_compile\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _disable_dynamo  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m   2103\u001b[39m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[32m   2104\u001b[39m \u001b[38;5;66;03m# Import interface functions defined in Python\u001b[39;00m\n\u001b[32m   2105\u001b[39m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[32m   2106\u001b[39m \n\u001b[32m   2107\u001b[39m \u001b[38;5;66;03m# needs to be after the above ATen bindings so we can overwrite from Python side\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2108\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _VF \u001b[38;5;28;01mas\u001b[39;00m _VF, functional \u001b[38;5;28;01mas\u001b[39;00m functional  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m   2109\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[32m   2111\u001b[39m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[32m   2112\u001b[39m \u001b[38;5;66;03m# Remove unnecessary members\u001b[39;00m\n\u001b[32m   2113\u001b[39m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projekty Programistyczne\\NLP-Cyber-NER\\.venv\\Lib\\site-packages\\torch\\functional.py:7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, List, Optional, Sequence, Tuple, TYPE_CHECKING, Union\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _VF, Tensor\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _add_docstr\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projekty Programistyczne\\NLP-Cyber-NER\\.venv\\Lib\\site-packages\\torch\\nn\\__init__.py:8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# mypy: allow-untyped-defs\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparameter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m      3\u001b[39m     Buffer \u001b[38;5;28;01mas\u001b[39;00m Buffer,\n\u001b[32m      4\u001b[39m     Parameter \u001b[38;5;28;01mas\u001b[39;00m Parameter,\n\u001b[32m      5\u001b[39m     UninitializedBuffer \u001b[38;5;28;01mas\u001b[39;00m UninitializedBuffer,\n\u001b[32m      6\u001b[39m     UninitializedParameter \u001b[38;5;28;01mas\u001b[39;00m UninitializedParameter,\n\u001b[32m      7\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodules\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     10\u001b[39m     attention \u001b[38;5;28;01mas\u001b[39;00m attention,\n\u001b[32m     11\u001b[39m     functional \u001b[38;5;28;01mas\u001b[39;00m functional,\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     utils \u001b[38;5;28;01mas\u001b[39;00m utils,\n\u001b[32m     17\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparallel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataParallel \u001b[38;5;28;01mas\u001b[39;00m DataParallel\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projekty Programistyczne\\NLP-Cyber-NER\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py:35\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      4\u001b[39m     CELU,\n\u001b[32m      5\u001b[39m     ELU,\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     Threshold,\n\u001b[32m     33\u001b[39m )\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01madaptive\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AdaptiveLogSoftmaxWithLoss\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbatchnorm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     36\u001b[39m     BatchNorm1d,\n\u001b[32m     37\u001b[39m     BatchNorm2d,\n\u001b[32m     38\u001b[39m     BatchNorm3d,\n\u001b[32m     39\u001b[39m     LazyBatchNorm1d,\n\u001b[32m     40\u001b[39m     LazyBatchNorm2d,\n\u001b[32m     41\u001b[39m     LazyBatchNorm3d,\n\u001b[32m     42\u001b[39m     SyncBatchNorm,\n\u001b[32m     43\u001b[39m )\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchannelshuffle\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChannelShuffle\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     46\u001b[39m     Container,\n\u001b[32m     47\u001b[39m     ModuleDict,\n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m     Sequential,\n\u001b[32m     52\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projekty Programistyczne\\NLP-Cyber-NER\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F, init\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparameter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Parameter, UninitializedBuffer, UninitializedParameter\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SyncBatchNorm \u001b[38;5;28;01mas\u001b[39;00m sync_batch_norm\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlazy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LazyModuleMixin\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodule\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Module\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projekty Programistyczne\\NLP-Cyber-NER\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\_functions.py:4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdist\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautograd\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Function\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mSyncBatchNorm\u001b[39;00m(Function):\n\u001b[32m      8\u001b[39m     \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m     10\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m         world_size,\n\u001b[32m     20\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projekty Programistyczne\\NLP-Cyber-NER\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cast, List, Optional, Sequence, Tuple, Union\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _vmap_internals\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moverrides\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m handle_torch_function, has_torch_function, is_tensor_like\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _size, _TensorOrTensors, _TensorOrTensorsOrGradEdge\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projekty Programistyczne\\NLP-Cyber-NER\\.venv\\Lib\\site-packages\\torch\\_vmap_internals.py:8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pytree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _broadcast_to_and_flatten, tree_flatten, tree_unflatten\n\u001b[32m     11\u001b[39m in_dims_t = Union[\u001b[38;5;28mint\u001b[39m, Tuple]\n\u001b[32m     12\u001b[39m out_dims_t = Union[\u001b[38;5;28mint\u001b[39m, Tuple[\u001b[38;5;28mint\u001b[39m, ...]]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projekty Programistyczne\\NLP-Cyber-NER\\.venv\\Lib\\site-packages\\torch\\utils\\_pytree.py:176\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;66;03m# NB: we try really hard to not import _cxx_pytree (which depends on optree)\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# as much as possible. This is for isolation: a user who is not using C++ pytree\u001b[39;00m\n\u001b[32m    174\u001b[39m \u001b[38;5;66;03m# shouldn't pay for it, and it helps makes things like cpython upgrades easier.\u001b[39;00m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     _optree_version = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moptree\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m importlib.metadata.PackageNotFoundError:\n\u001b[32m    178\u001b[39m     _cxx_pytree_dynamo_traceable = _cxx_pytree_exists = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\metadata\\__init__.py:889\u001b[39m, in \u001b[36mversion\u001b[39m\u001b[34m(distribution_name)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mversion\u001b[39m(distribution_name):\n\u001b[32m    883\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[32m    884\u001b[39m \n\u001b[32m    885\u001b[39m \u001b[33;03m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[32m    886\u001b[39m \u001b[33;03m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[32m    887\u001b[39m \u001b[33;03m        \"Version\" metadata key.\u001b[39;00m\n\u001b[32m    888\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m889\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m.version\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\metadata\\__init__.py:862\u001b[39m, in \u001b[36mdistribution\u001b[39m\u001b[34m(distribution_name)\u001b[39m\n\u001b[32m    856\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdistribution\u001b[39m(distribution_name):\n\u001b[32m    857\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[32m    858\u001b[39m \n\u001b[32m    859\u001b[39m \u001b[33;03m    :param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[32m    860\u001b[39m \u001b[33;03m    :return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[32m    861\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m862\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDistribution\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\metadata\\__init__.py:397\u001b[39m, in \u001b[36mDistribution.from_name\u001b[39m\u001b[34m(cls, name)\u001b[39m\n\u001b[32m    395\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mA distribution name is required.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdiscover\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projekty Programistyczne\\NLP-Cyber-NER\\.venv\\Lib\\site-packages\\importlib_metadata\\__init__.py:931\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Find metadata directories in paths heuristically.\"\"\"\u001b[39;00m\n\u001b[32m    929\u001b[39m prepared = Prepared(name)\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m itertools.chain.from_iterable(\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m     \u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepared\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(FastPath, paths)\n\u001b[32m    932\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projekty Programistyczne\\NLP-Cyber-NER\\.venv\\Lib\\site-packages\\importlib_metadata\\__init__.py:790\u001b[39m, in \u001b[36mFastPath.search\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msearch\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lookup(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmtime\u001b[49m).search(name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projekty Programistyczne\\NLP-Cyber-NER\\.venv\\Lib\\site-packages\\importlib_metadata\\__init__.py:794\u001b[39m, in \u001b[36mFastPath.mtime\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    792\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    793\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmtime\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m suppress(\u001b[38;5;167;01mOSError\u001b[39;00m):\n\u001b[32m    795\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m os.stat(\u001b[38;5;28mself\u001b[39m.root).st_mtime\n\u001b[32m    796\u001b[39m     \u001b[38;5;28mself\u001b[39m.lookup.cache_clear()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:446\u001b[39m, in \u001b[36msuppress.__exit__\u001b[39m\u001b[34m(self, exctype, excinst, exctb)\u001b[39m\n\u001b[32m    443\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    444\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exctype, excinst, exctb):\n\u001b[32m    447\u001b[39m     \u001b[38;5;66;03m# Unlike isinstance and issubclass, CPython exception handling\u001b[39;00m\n\u001b[32m    448\u001b[39m     \u001b[38;5;66;03m# currently only looks at the concrete type hierarchy (ignoring\u001b[39;00m\n\u001b[32m    449\u001b[39m     \u001b[38;5;66;03m# the instance and subclass checking hooks). While Guido considers\u001b[39;00m\n\u001b[32m    450\u001b[39m     \u001b[38;5;66;03m# that a bug rather than a feature, it's a fairly hard one to fix\u001b[39;00m\n\u001b[32m    451\u001b[39m     \u001b[38;5;66;03m# due to various internal implementation details. suppress provides\u001b[39;00m\n\u001b[32m    452\u001b[39m     \u001b[38;5;66;03m# the simpler issubclass based semantics, rather than trying to\u001b[39;00m\n\u001b[32m    453\u001b[39m     \u001b[38;5;66;03m# exactly reproduce the limitations of the CPython interpreter.\u001b[39;00m\n\u001b[32m    454\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    455\u001b[39m     \u001b[38;5;66;03m# See http://bugs.python.org/issue12029 for more details\u001b[39;00m\n\u001b[32m    456\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exctype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    457\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from nlp_cyber_ner.dataset import read_iob2_file, clean_dnrti, unify_labels_dnrti\n",
    "dnrti_path = RAW_DATA_DIR / \"DNRTI\"\n",
    "dnrti_train_path = dnrti_path / \"train.txt\"\n",
    "dnrti_dev_path = dnrti_path / \"valid.txt\"\n",
    "dnrti_test_path = dnrti_path / \"test.txt\"\n",
    "clean_dnrti(dnrti_train_path)\n",
    "clean_dnrti(dnrti_dev_path)\n",
    "clean_dnrti(dnrti_test_path)\n",
    "# move to interim folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dnrti_path = INTERIM_DATA_DIR / \"DNRTI\"\n",
    "dnrti_train_path = dnrti_path / \"train.cleaned\"\n",
    "dnrti_dev_path = dnrti_path / \"valid.cleaned\"\n",
    "dnrti_test_path = dnrti_path / \"test.cleaned\"\n",
    "unify_labels_dnrti(dnrti_train_path)\n",
    "unify_labels_dnrti(dnrti_dev_path)\n",
    "unify_labels_dnrti(dnrti_test_path)\n",
    "# move to processed folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnrti_path = PROCESSED_DATA_DIR / \"dnrti\"\n",
    "dnrti_train_path = dnrti_path / \"train.unified\"\n",
    "dnrti_dev_path = dnrti_path / \"valid.unified\"\n",
    "dnrti_test_path = dnrti_path / \"test.unified\"\n",
    "\n",
    "dnrti_train_data = read_iob2_file(dnrti_train_path, word_index=0, tag_index=1)\n",
    "dnrti_dev_data = read_iob2_file(dnrti_dev_path, word_index=0, tag_index=1)\n",
    "dnrti_test_data = read_iob2_file(dnrti_test_path, word_index=0, tag_index=1)\n",
    "end_labels = {'B-Organization', 'O', 'I-Malware', 'B-System', 'I-Vulnerability', 'I-Organization', 'I-System', 'B-Vulnerability', 'B-Malware'}\n",
    "A = set(tag for _, tags in dnrti_train_data for tag in tags)\n",
    "B = set(tag for _, tags in dnrti_dev_data for tag in tags)\n",
    "C = set(tag for _, tags in dnrti_test_data for tag in tags)\n",
    "assert A == B == C == end_labels, \"The labels in the train, dev and test sets are not the same.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnrti_pack = transform_dataset(\n",
    "    dnrti_train_data, dnrti_dev_data, dnrti_test_data, True\n",
    ")\n",
    "dnrti_train_X, dnrti_train_y, dnrti_dev_X, dnrti_dev_y, dnrti_test_X, dnrti_test_y, dnrti_idx2word, dnrti_idx2label, dnrti_max_len = dnrti_pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 2,  ..., 0, 0, 0],\n",
       "        [2, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 6, 1,  ..., 0, 0, 0],\n",
       "        [2, 1, 1,  ..., 0, 0, 0],\n",
       "        [2, 1, 1,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnrti_test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuda, MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/PLtier/NLP-Cyber-NER.mlflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_cyber_ner.dataset import get_labels, preds_to_tags\n",
    "from nlp_cyber_ner.span_f1 import span_f1\n",
    "import gc\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def train_and_eval(\n",
    "    train_X: torch.Tensor,\n",
    "    train_y: torch.Tensor,\n",
    "    dev_X: torch.Tensor,\n",
    "    dev_labels: list[tuple[str]],\n",
    "    idx2word: list[str],\n",
    "    idx2label: list[str],\n",
    "    max_len: int,\n",
    ") -> None:\n",
    "\n",
    "\n",
    "    ### Transforms\n",
    "    # !nvidia-smi\n",
    "    # put already to gpu if having space:\n",
    "    train_X, train_y = train_X.to(device), train_y.to(device)\n",
    "    dev_X = dev_X.to(device)\n",
    "    ### Batching\n",
    "\n",
    "    # TODO: Maybe dtype would need to be changed!\n",
    "    BATCH_SIZE = 32\n",
    "    train_dataset = TensorDataset(train_X, train_y)\n",
    "    train_loader = DataLoader(train_dataset, BATCH_SIZE)  # drop_last=True\n",
    "    n_batches = len(train_loader)\n",
    "    ### Training\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    DIM_EMBEDDING = 100\n",
    "    LSTM_HIDDEN = 100\n",
    "    BATCH_SIZE = 32\n",
    "    LEARNING_RATE = 0.01\n",
    "    EPOCHS = 15\n",
    "\n",
    "\n",
    "    class TaggerModel(torch.nn.Module):\n",
    "        def __init__(self, nwords, ntags):\n",
    "            super().__init__()\n",
    "            # TODO Do Bidirectional LSTM\n",
    "            self.embed = nn.Embedding(nwords, DIM_EMBEDDING)\n",
    "            self.drop1 = nn.Dropout(p=0.2)\n",
    "            self.rnn = nn.LSTM(\n",
    "                DIM_EMBEDDING, LSTM_HIDDEN, batch_first=True, bidirectional=True\n",
    "            )\n",
    "            self.drop2 = nn.Dropout(p=0.3)\n",
    "            self.fc = nn.Linear(LSTM_HIDDEN * 2, ntags)\n",
    "\n",
    "        def forward(self, input_data):\n",
    "            word_vectors = self.embed(input_data)\n",
    "            regular1 = self.drop1(word_vectors)\n",
    "            output, hidden = self.rnn(regular1)\n",
    "            regular2 = self.drop2(output)\n",
    "\n",
    "            predictions = self.fc(regular2)\n",
    "            return predictions\n",
    "\n",
    "\n",
    "    model = TaggerModel(len(idx2word), len(idx2label))\n",
    "    model = model.to(device)  # run on cuda if possible\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_function = torch.nn.CrossEntropyLoss(ignore_index=0, reduction=\"sum\")\n",
    "\n",
    "    # creating the batches\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        # reset the gradient\n",
    "        print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "        loss_sum = 0\n",
    "\n",
    "        # loop over batches\n",
    "        # types for convenience\n",
    "        batch_X: torch.Tensor\n",
    "        batch_y: torch.Tensor\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            # TODO: if having memory issues comment .to(device)\n",
    "            # from one of the previous cells, and uncomment that:\n",
    "            # batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predicted_values = model.forward(batch_X)\n",
    "\n",
    "            # Cross entropy request (predictions, classes) shape for predictions, and (classes) for batch_y\n",
    "\n",
    "            # calculate loss\n",
    "            loss = loss_function(\n",
    "                predicted_values.view(batch_X.shape[0] * max_len, -1), batch_y.flatten()\n",
    "            )  # TODO: Last batch has 31 entries instead of 32 - we don't adjust much for that.\n",
    "            loss_sum += loss.item()  # avg later\n",
    "\n",
    "            # update\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Average loss after epoch {epoch + 1}: {loss_sum / n_batches}\")\n",
    "\n",
    "    # set to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # eval using Span_F1\n",
    "    predictions_dev = model.forward(dev_X)\n",
    "    print(predictions_dev.shape)\n",
    "    # gives probabilities for each tag (dim=18) for each word/feature (dim=159) for each sentence(dim=2000)\n",
    "    # we want to classify each word for the part-of-speech with highest probability\n",
    "    labels_dev = torch.argmax(predictions_dev, 2)\n",
    "    print(labels_dev.shape)\n",
    "\n",
    "    labels_dev = preds_to_tags(idx2label, dev_labels,labels_dev )\n",
    "\n",
    "    metrics = span_f1(dev_labels, labels_dev)\n",
    "\n",
    "    for k, v in metrics.items():\n",
    "        mlflow.log_metric(k, v) \n",
    "    \n",
    "\n",
    "\n",
    "    del predictions_dev\n",
    "    del labels_dev\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run train-cyner-eval-cyner at: https://dagshub.com/PLtier/NLP-Cyber-NER.mlflow/#/experiments/8/runs/be597c27b43a4839bab415bbf18c5f80\n",
      "🧪 View experiment at: https://dagshub.com/PLtier/NLP-Cyber-NER.mlflow/#/experiments/8\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m mlflow.start_run(run_name=name):\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(dev_y) == \u001b[38;5;28mlist\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDev y is not a list!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     \u001b[43mtrain_and_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdev_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdev_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_idx2word\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_idx2label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_max_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mtrain_and_eval\u001b[39m\u001b[34m(train_X, train_y, dev_X, dev_labels, idx2word, idx2label, max_len)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_and_eval\u001b[39m(\n\u001b[32m      9\u001b[39m     train_X: torch.Tensor,\n\u001b[32m     10\u001b[39m     train_y: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# !nvidia-smi\u001b[39;00m\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# put already to gpu if having space:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     train_X, train_y = \u001b[43mtrain_X\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, train_y.to(device)\n\u001b[32m     23\u001b[39m     dev_X = dev_X.to(device)\n\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m### Batching\u001b[39;00m\n\u001b[32m     25\u001b[39m \n\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# TODO: Maybe dtype would need to be changed!\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# SO the idea is: we have four 'packs' required for training.\n",
    "# I want to train the model on each pack's train set, and evaluate on the dev set of all four packs.\n",
    "# So, we have 4 train sets and 4 dev sets, everything logged to mlflow.\n",
    "\n",
    "from numpy.random import f\n",
    "\n",
    "\n",
    "train_packs = [\n",
    "    (\"cyner\", cyner_train_data),\n",
    "    (\"aptner\", aptner_train_data),\n",
    "    (\"attackner\", attackner_train_data),\n",
    "    (\"dnrti\", dnrti_train_data),\n",
    "]\n",
    "\n",
    "dev_packs = [\n",
    "(\"cyner\", cyner_dev_data),\n",
    "    (\"aptner\", aptner_dev_data),\n",
    "    (\"attackner\", attackner_dev_data),\n",
    "    (\"dnrti\", dnrti_dev_data),\n",
    "]\n",
    "\n",
    "for train_pack_name, train_pack in train_packs:\n",
    "    for dev_pack_name, dev_pack in train_packs:\n",
    "        name: str = f\"train-{train_pack_name}-eval-{dev_pack_name}\"\n",
    "        data_pack = transform_dataset(\n",
    "            train_pack, dev_pack, train_pack, True #the third one is not used, but needed for the function to work\n",
    "        )\n",
    "\n",
    "        # train_X, train_y, dev_X, dev_y, test_X, test_y, idx2word, idx2label, max_len = train_pack\n",
    "        train_X, train_y, dev_X, dev_y, _, _, train_idx2word, train_idx2label, train_max_len = data_pack\n",
    "        mlflow.set_experiment(name)\n",
    "        with mlflow.start_run(run_name=name):\n",
    "            assert type(dev_y) == list, \"Dev y is not a list!\"\n",
    "            train_and_eval(\n",
    "                train_X,\n",
    "                train_y,\n",
    "                dev_X,\n",
    "                dev_y,\n",
    "                train_idx2word,\n",
    "                train_idx2label,\n",
    "                train_max_len,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop:\n",
    "# so I want to obtain 4x4 results: i.e. for each pair of train/valid (no test) datasets from my four\n",
    "# datasets, I want to train a model and evaluate it on the other two datasets. I will then have 4x4=16 results.\n",
    "# let's go:\n",
    "\n",
    "\n",
    "#mlflow experiments - a simple on dnrti data\n",
    "mlflow.set_experiment(\"dnrti\")\n",
    "with mlflow.start_run(run_name=\"dnrti\") as run:\n",
    "    train_and_eval(\n",
    "        dnrti_train_X,\n",
    "        dnrti_train_y,\n",
    "        dnrti_dev_X,\n",
    "        dnrti_dev,\n",
    "        dnrti_idx2word,\n",
    "        dnrti_idx2label,\n",
    "        dnrti_max_len,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/03 18:35:02 INFO mlflow.tracking.fluent: Experiment with name 'train-cyner-eval-cyner' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Average loss after epoch 1: 286.313142971559\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Average loss after epoch 2: 133.6077549511736\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "🏃 View run train-cyner-eval-cyner at: https://dagshub.com/PLtier/NLP-Cyber-NER.mlflow/#/experiments/8/runs/ad13fea49d0c477fabf251ee4ee4ceaa\n",
      "🧪 View experiment at: https://dagshub.com/PLtier/NLP-Cyber-NER.mlflow/#/experiments/8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m mlflow.start_run(run_name=\u001b[33m\"\u001b[39m\u001b[33mtrain-cyner-eval-cyner\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m run:\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(cyner_dev_y) == \u001b[38;5;28mlist\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcyner_dev_y is not a list of tuples, make sure to pass True to transform_dataset\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mtrain_and_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcyner_train_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcyner_train_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcyner_dev_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcyner_dev_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcyner_idx2word\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcyner_idx2label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcyner_max_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 87\u001b[39m, in \u001b[36mtrain_and_eval\u001b[39m\u001b[34m(train_X, train_y, dev_X, dev_labels, idx2word, idx2label, max_len)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_X, batch_y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m     81\u001b[39m     \u001b[38;5;66;03m# TODO: if having memory issues comment .to(device)\u001b[39;00m\n\u001b[32m     82\u001b[39m     \u001b[38;5;66;03m# from one of the previous cells, and uncomment that:\u001b[39;00m\n\u001b[32m     83\u001b[39m     \u001b[38;5;66;03m# batch_X, batch_y = batch_X.to(device), batch_y.to(device)\u001b[39;00m\n\u001b[32m     85\u001b[39m     optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     predicted_values = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m     \u001b[38;5;66;03m# Cross entropy request (predictions, classes) shape for predictions, and (classes) for batch_y\u001b[39;00m\n\u001b[32m     90\u001b[39m \n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# calculate loss\u001b[39;00m\n\u001b[32m     92\u001b[39m     loss = loss_function(\n\u001b[32m     93\u001b[39m         predicted_values.view(batch_X.shape[\u001b[32m0\u001b[39m] * max_len, -\u001b[32m1\u001b[39m), batch_y.flatten()\n\u001b[32m     94\u001b[39m     )  \u001b[38;5;66;03m# TODO: Last batch has 31 entries instead of 32 - we don't adjust much for that.\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mtrain_and_eval.<locals>.TaggerModel.forward\u001b[39m\u001b[34m(self, input_data)\u001b[39m\n\u001b[32m     54\u001b[39m word_vectors = \u001b[38;5;28mself\u001b[39m.embed(input_data)\n\u001b[32m     55\u001b[39m regular1 = \u001b[38;5;28mself\u001b[39m.drop1(word_vectors)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m output, hidden = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregular1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m regular2 = \u001b[38;5;28mself\u001b[39m.drop2(output)\n\u001b[32m     59\u001b[39m predictions = \u001b[38;5;28mself\u001b[39m.fc(regular2)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Maciej\\coding-projects\\NLP-Cyber-NER\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Maciej\\coding-projects\\NLP-Cyber-NER\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Maciej\\coding-projects\\NLP-Cyber-NER\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1124\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1121\u001b[39m         hx = \u001b[38;5;28mself\u001b[39m.permute_hidden(hx, sorted_indices)\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1124\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1128\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1136\u001b[39m     result = _VF.lstm(\n\u001b[32m   1137\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1138\u001b[39m         batch_sizes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1145\u001b[39m         \u001b[38;5;28mself\u001b[39m.bidirectional,\n\u001b[32m   1146\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# loop:\n",
    "# so I want to obtain 4x4 results: i.e. for each pair of train/valid (no test) datasets from my four\n",
    "# datasets, I want to train a model and evaluate it on the other two datasets. I will then have 4x4=16 results.\n",
    "# let's go:\n",
    "\n",
    "\n",
    "#mlflow experiments - a simple on dnrti data\n",
    "mlflow.set_experiment(\"train-cyner-eval-cyner\")\n",
    "with mlflow.start_run(run_name=\"train-cyner-eval-cyner\") as run:\n",
    "    assert type(cyner_dev_y) == list, \"cyner_dev_y is not a list of tuples, make sure to pass True to transform_dataset\"\n",
    "    train_and_eval(\n",
    "        cyner_train_X,\n",
    "        cyner_train_y,\n",
    "        cyner_dev_X,\n",
    "        cyner_dev_y,\n",
    "        cyner_idx2word,\n",
    "        cyner_idx2label,\n",
    "        cyner_max_len,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save test for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "from nlp_cyber_ner.dataset import prepare_output_file\n",
    "\n",
    "# Evaluating on dev data we will predict using trained TaggerModel\n",
    "predictions_test = model.forward(test_X)\n",
    "print(predictions_test.shape)\n",
    "# gives probabilities for each tag (dim=18) for each word/feature (dim=159) for each sentence(dim=2000)\n",
    "# we want to classify each word for the part-of-speech with highest probability\n",
    "labels_test = torch.argmax(predictions_test, 2)\n",
    "print(labels_test.shape)\n",
    "### save labels\n",
    "prepare_output_file(\n",
    "    transformer, test_data, labels_test, \"./en_ewt-ud-test-masked.iob2\", \"./test.iob2\"\n",
    ")\n",
    "\n",
    "del predictions_test\n",
    "del labels_test\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Project proposal\n",
    "\n",
    "The written proposal should consist of maximum one page in [ACL-format](https://github.com/acl-org/acl-style-files) (The bibliography does not count for the word limit). In here, you should explain the last three points from the list above and place your project in a larger context (previous work).\n",
    "\n",
    "Make sure your proposal is:\n",
    "* Novel to some extent\n",
    "* Doable within the time-frame\n",
    "\n",
    "*hint* The [ACL Anthology](https://aclanthology.org/) contains almost all peer-reviewed NLP papers.\n",
    "\n",
    "**Deadline: 03-04 on LearnIt (14:00)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Final project\n",
    "The final project has a maximum size of 5 pages (excluding bibliography and appendix), using the [ACL style files](https://github.com/acl-org/acl-style-files)\n",
    "\n",
    "Besides the main paper (discussed in class), you have to include:\n",
    "* Group contributions. State who was responsible for which part of the project. Here you may state if there\n",
    "were any serious unequal workloads among group members. This should be put in the appendix.\n",
    "* A report on usage of chatbots. We follow: https://2023.aclweb.org/blog/ACL-2023-policy/\n",
    "   * Add a section in appendix if you made use of a chatbot (since we do not use a Responsible NLP Checklist)\n",
    "   * Include each stage on the ACL policy, and indicate to what extent you used a chatbot\n",
    "   * Use with care!, you are responsible for the project and plagiarism, correctness etc.\n",
    "\n",
    "You can also put additional results and details in the appendix. However, the paper itself should be standalone, and understandable without consulting the appendix.\n",
    "\n",
    "Furthermore, the code should be available on www.github.itu.dk (with a link in a footnote at the end of the abstract) , it should include a README with instructions on how to reproduce your results.\n",
    "\n",
    "**Deadline: 23-05 on LearnIt** Please check the checklist below before uploading!\n",
    "\n",
    "Optionally, you can upload a draft a week before **16-05 (before 09:00)** for an extra round of feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Analysis is essential for the interpretation of your results. In this section we will shortly describe some different types of analysis. We strongly suggest to use at least one of these:\n",
    "\n",
    "* **Ablation study**: Leave out a certain part of the model, to study its effects. For example, disable the tokenizer, remove a certain (group of) feature(s), or disable the stop-word removal. If the performance drops a lot, it means that this part of the model contributes heavily to the models final performance. This is commonly done in 1 table, while disabling different parts of the model. Note that you can also do this the other way around, i.e. use only one feature (group) at a time, and test performance\n",
    "* **Learning curve**: Evaluate how much data your model needs to reach a certain performance. Especially for the data augmentation projects this is essential.\n",
    "* **Quantitative analysis**: Automated means of analyzing in which cases your model performs worse. This can for example be done with a confusion matrix.\n",
    "* **Qualitative analysis**: Manually inspect a certain number of errors, and try to categorize them/find trends. Can be combined with the quantitative analysis, i.e., inspect 100 cases of positive reviews predicted to be negative and 100 cases of negative reviews predicted to be positive\n",
    "* **Feature importance**: In traditional machine learning methods, one can often extract and inspect the weights of the features. In sklearn these can be found in: `trained_model.coef_`\n",
    "* **Other metrics**: per class scores, partial matches, or count how often the span-borders were correct, but the label wrong.\n",
    "* **Input words importance**: To gain insight into which words have a impact on prediction performance (positive, negative), we can analyze per-word impact: given a trained model, replace a given word with\n",
    "the unknown word token and observe the change in prediction score (probability for a class). This is\n",
    "shown in Figure 4 of [Rethmeier et al (2018)](https://aclweb.org/anthology/W18-6246) (a paper on controversy detection), also shown below: red-colored\n",
    "tokens were important for controversy detection, blue-colored token decreased prediction scores.\n",
    "\n",
    "<img width=400px src=example.png>\n",
    "\n",
    "Note that this is a non-exhaustive list, and you are encouraged to also explore additional analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checklist final project\n",
    "Please check all these items before handing in your final report. You only have to upload a pdf file on learnit, and make sure a link to the code is included in the report and the code is accesible. \n",
    "\n",
    "* Are all group members and their email addresses specified?\n",
    "* Does the group report include a representative project title?\n",
    "* Does the group report contain an abstract?\n",
    "* Does the introduction clearly specify the research intention and research question?\n",
    "* Does the group report adequately refer to the relevant literature?\n",
    "* Does the group report properly use figure, tables and examples?\n",
    "* Does the group report provide and discuss the empirical results?\n",
    "* Is the group report proofread?\n",
    "* Does the pdf contain the link to the project’s github repo?\n",
    "* Is the github repo accessible to the public (within ITU)?\n",
    "* Is the group report maximum 5 pages long, excluding references and appendix?\n",
    "* Are the group contributions added in the appendix?\n",
    "* Does the repository contain all scripts and code to reproduce the results in the group report? Are instructions\n",
    " provided on how to run the code?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (desktop)",
   "language": "python",
   "name": "desktops"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
