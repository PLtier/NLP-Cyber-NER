{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import jsonlines\n",
    "import torch\n",
    "\n",
    "from nlp_cyber_ner.dataset import read_cyner, read_aptner, read_attacker, read_dnrti\n",
    "from nlp_cyber_ner.dataset import read_iob2_file\n",
    "from nlp_cyber_ner.dataset import unify_labels_aptner\n",
    "from nlp_cyber_ner.dataset import clean_aptner, clean_dnrti\n",
    "from nlp_cyber_ner.dataset import transform_dataset\n",
    "from nlp_cyber_ner.config import PROCESSED_DATA_DIR, RAW_DATA_DIR, INTERIM_DATA_DIR, TOKENPROCESSED_DATA_DIR\n",
    "from nlp_cyber_ner.dataset import Preprocess \n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For reference, the code that produces the data in tokenprocessed is in this notebook. The training scripts for the tokenmodels expect this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### APTNER: Using the labels that we ended up mapping to CYNER as well as some of the ones that aren't trivial to solve (e.g. ip.address) such that the datasets maintain their identity more.\n",
    "\n",
    "Btw, I think, given the description of vulnerability in CYNER, \"Vulnerability includes both CVE ID (e.g., CVE-2012-2825) and mention of exploits (e.g., master key vulnerability).\", we should keep VULID and map it to Vulnerability in CYNER - In the datasets where we actually do mapping, not relevant here, just wanted to write it down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is just a repurposed unify_labels_aptner() function, but changed so that it drops instead of doing any mapping\n",
    "\n",
    "def drop_irrelevant_aptner_labels(path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Reads in APTNer in its cleaned iob2-esque format (prefixes don't follow iob2 quite yet), and writes to iob2 format, dropping irrelevant labels.\n",
    "    All E- labels are converted to I- labels.\n",
    "    All S- labels are converted to B- labels.\n",
    "    MD5, SHA1, SHA2, LOC, TIME, IP are dropped, everything else is kept.\n",
    "    \"\"\"\n",
    "\n",
    "    with (\n",
    "        open(path, \"r\", encoding=\"utf-8\") as f,\n",
    "        open(path.with_suffix(\".tokenready\"), \"w\", encoding=\"utf-8\") as f_out,\n",
    "    ):\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                print(line)\n",
    "                tok = line.split()\n",
    "                assert len(tok) == 2\n",
    "                new_tag = tok[1]\n",
    "                if tok[1] != \"O\":\n",
    "                    prefix, label = tok[1].split(\"-\")\n",
    "\n",
    "                    if (label == \"MD5\"\n",
    "                        or label == \"SHA1\"\n",
    "                        or label == \"SHA2\" \n",
    "                        or label == \"LOC\"\n",
    "                        or label == \"TIME\"\n",
    "                        or label == \"IP\"):\n",
    "                        label = \"O\"\n",
    "                        f_out.write(f\"{tok[0]} O\\n\")\n",
    "                        continue\n",
    "\n",
    "                    if prefix == \"E\":\n",
    "                        prefix = \"I\"\n",
    "                    elif prefix == \"S\":\n",
    "                        prefix = \"B\"\n",
    "\n",
    "                    new_tag = f\"{prefix}-{label}\"\n",
    "                f_out.write(f\"{tok[0]} {new_tag}\\n\")\n",
    "            else:\n",
    "                f_out.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aptner_path = INTERIM_DATA_DIR / \"APTNer\"\n",
    "aptner_train_path= aptner_path / \"APTNERtrain.cleaned\"\n",
    "aptner_dev_path= aptner_path / \"APTNERdev.cleaned\"\n",
    "aptner_test_path= aptner_path / \"APTNERtest.cleaned\"\n",
    "\n",
    "drop_irrelevant_aptner_labels(aptner_train_path)\n",
    "drop_irrelevant_aptner_labels(aptner_dev_path)\n",
    "drop_irrelevant_aptner_labels(aptner_test_path)\n",
    "\n",
    "# move manually to tokenprocessed after this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aptner_path = TOKENPROCESSED_DATA_DIR / \"APTNer\"\n",
    "aptner_train_path= aptner_path / \"train.tokenready\"\n",
    "aptner_dev_path= aptner_path / \"valid.tokenready\"\n",
    "aptner_test_path= aptner_path / \"test.tokenready\"\n",
    "aptner_train_data = read_iob2_file(aptner_train_path)\n",
    "aptner_dev_data = read_iob2_file(aptner_dev_path)\n",
    "aptner_test_data = read_iob2_file(aptner_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "aptner_train_X, aptner_train_y, aptner_dev_X, aptner_dev_y, aptner_test_X, aptner_test_y, aptner_idx2word, aptner_idx2label, aptner_max_len = \\\n",
    "transform_dataset(\n",
    "    aptner_train_data, aptner_dev_data, aptner_test_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7679, 82])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aptner_train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aptner_idx2label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNRTI: Same approach as for APTNER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_irrelevant_dnrti_labels(path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Reads in DNRTI in iob2 format, writes to iob2 format again, dropping irrelevant labels.\n",
    "    Seems we agreed to drop Way, Area, Purp, Exp, Features, some time ago, so I'm dropping those here, but keeping the rest.\n",
    "    This, like for aptner, includes some labels that were not originally mapped to CYNER.\n",
    "    \"\"\"\n",
    "\n",
    "    with (\n",
    "        open(path, \"r\", encoding=\"utf-8\") as f,\n",
    "        open(path.with_suffix(\".tokenready\"), \"w\", encoding=\"utf-8\") as f_out,\n",
    "    ):\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                tok = line.split()\n",
    "                assert len(tok) == 2\n",
    "                new_tag = tok[1]\n",
    "                if tok[1] != \"O\":\n",
    "                    prefix, label = tok[1].split(\"-\")\n",
    "                    if (label == \"Way\"\n",
    "                        or label == \"Area\"\n",
    "                        or label == \"Purp\"\n",
    "                        or label == \"Exp\"\n",
    "                        or label == \"Features\"):\n",
    "                        label = \"O\"\n",
    "                        f_out.write(f\"{tok[0]} O\\n\")\n",
    "                        continue\n",
    "                    new_tag = f\"{prefix}-{label}\"\n",
    "                f_out.write(f\"{tok[0]} {new_tag}\\n\")\n",
    "            else:\n",
    "                f_out.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnrti_path = INTERIM_DATA_DIR / \"DNRTI\"\n",
    "dnrti_train_path = dnrti_path / \"train.cleaned\"\n",
    "dnrti_dev_path = dnrti_path / \"valid.cleaned\"\n",
    "dnrti_test_path = dnrti_path / \"test.cleaned\"\n",
    "\n",
    "#fun\n",
    "drop_irrelevant_dnrti_labels(dnrti_train_path)\n",
    "drop_irrelevant_dnrti_labels(dnrti_dev_path)\n",
    "drop_irrelevant_dnrti_labels(dnrti_test_path)\n",
    "\n",
    "# move manually to tokenprocessed after this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnrti_path = TOKENPROCESSED_DATA_DIR / \"DNRTI\"\n",
    "dnrti_train_path = dnrti_path / \"train.tokenready\"\n",
    "dnrti_dev_path = dnrti_path / \"valid.tokenready\"\n",
    "dnrti_test_path = dnrti_path / \"test.tokenready\"\n",
    "\n",
    "dnrti_train_data = read_iob2_file(dnrti_train_path, word_index=0, tag_index=1)\n",
    "dnrti_dev_data = read_iob2_file(dnrti_dev_path, word_index=0, tag_index=1)\n",
    "dnrti_test_data = read_iob2_file(dnrti_test_path, word_index=0, tag_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnrti_train_X, dnrti_train_y, dnrti_dev_X, dnrti_dev_y, dnrti_test_X, dnrti_test_y, dnrti_idx2word, dnrti_idx2label, dnrti_max_len = \\\n",
    "transform_dataset(\n",
    "    dnrti_train_data, dnrti_dev_data, dnrti_test_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5251, 82])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnrti_train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnrti_idx2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For ATTACKNER, most labels don't seem trivial or irrelevant, except for location, which can probably be argued to be accomplished by more general NER models - I'll drop that here to stay consistent with what was dropped for DNRTI and APTNER. Everything else stays as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the function from nlp_cyber_ner.dataset.py to get the attacker dataset in IOB2 format. Just repurposing so there is no mapping/merging \n",
    "# with CYNER \n",
    "\n",
    "def attacker_to_iob2_format(path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Keeping the original labels (except dropping 11/18!)\n",
    "    Outputs a conll/iob2 format\n",
    "    \"\"\"\n",
    "    with (\n",
    "        jsonlines.open(path) as reader,\n",
    "        open(path.with_suffix(\".tokenready\"), \"w\", encoding=\"utf-8\") as f_out,\n",
    "    ):\n",
    "        for obj in reader:\n",
    "            tags = obj[\"tags\"]\n",
    "            tokens = obj[\"tokens\"]\n",
    "            n = len(tokens)\n",
    "            for i in range(n):\n",
    "                current_tag = tags[i]\n",
    "                token = tokens[i]\n",
    "                if token == \" \":\n",
    "                    # TODO: this is kind of cleaning part, if there is time, I would put it in a separate function\n",
    "                    continue\n",
    "                if current_tag != \"O\":\n",
    "                    prefix, label = current_tag.split(\"-\")\n",
    "                    if label == \"LOCATION\": \n",
    "                        label = \"O\"\n",
    "                        f_out.write(f\"{token} O\\n\")\n",
    "                        continue\n",
    "                    current_tag = f\"{prefix}-{label}\"\n",
    "                f_out.write(f\"{token} {current_tag}\\n\")\n",
    "            f_out.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "attackner_path = RAW_DATA_DIR / \"attackner\"\n",
    "attackner_train_path  = attackner_path / \"train.json\"\n",
    "attackner_dev_path= attackner_path / \"dev.json\"\n",
    "attackner_test_path= attackner_path / \"test.json\"\n",
    "\n",
    "\n",
    "attacker_to_iob2_format(attackner_test_path)\n",
    "attacker_to_iob2_format(attackner_train_path)\n",
    "attacker_to_iob2_format(attackner_dev_path)\n",
    "\n",
    "# move manually to tokenprocessed after this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "attackner_path = TOKENPROCESSED_DATA_DIR / \"attacker\"\n",
    "attackner_train_path  = attackner_path / \"train.tokenready\"\n",
    "attackner_dev_path= attackner_path / \"dev.tokenready\"\n",
    "attackner_test_path= attackner_path / \"test.tokenready\"\n",
    "\n",
    "attackner_train_data = read_iob2_file(attackner_train_path, word_index=0, tag_index=1)\n",
    "attackner_dev_data = read_iob2_file(attackner_dev_path, word_index=0, tag_index=1)\n",
    "attackner_test_data = read_iob2_file(attackner_test_path, word_index=0, tag_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "attackner_train_X, attackner_train_y, attackner_dev_X, attackner_dev_y, attackner_test_X, attackner_test_y, attackner_idx2word, attackner_idx2label, attackner_max_len = \\\n",
    "transform_dataset(\n",
    "    attackner_train_data, attackner_dev_data, attackner_test_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2481, 107])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attackner_train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attackner_idx2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CYNER - Doesn't require any processing specific to the token method - we keep indicator dropped for the same reasons as stated for other datasets; Indicator contains subcategories of entities, email, hash, port number, etc, that we believe to be trivial. So this is just loading in the processed cyner data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just download the bug fixed cyner datasets from github and manually replace for now.\n",
    "#There is no token specific version of cyner.\n",
    "\n",
    "cyner_path = PROCESSED_DATA_DIR / \"cyner\"\n",
    "cyner_train_path = cyner_path / \"train.unified\"\n",
    "cyner_dev_path = cyner_path / \"valid.unified\"\n",
    "cyner_test_path = cyner_path / \"test.unified\"\n",
    "cyner_train_data = read_iob2_file(cyner_train_path)\n",
    "cyner_dev_data = read_iob2_file(cyner_dev_path)\n",
    "cyner_test_data = read_iob2_file(cyner_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyner_train_X, cyner_train_y, cyner_dev_X, cyner_dev_y, cyner_test_X, cyner_test_y, cyner_idx2word, cyner_idx2label, cyner_max_len = \\\n",
    "transform_dataset(\n",
    "    cyner_train_data, cyner_dev_data, cyner_test_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<PAD>',\n",
       " 'B-Malware',\n",
       " 'I-Malware',\n",
       " 'O',\n",
       " 'B-System',\n",
       " 'I-System',\n",
       " 'B-Organization',\n",
       " 'I-Organization',\n",
       " 'B-Vulnerability',\n",
       " 'I-Vulnerability']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cyner_idx2label\n",
    "\n",
    "#bugged. Will need to change unify cyner function in dataset.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<PAD>', 'B-Malware']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cyner_idx2label[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7955"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cyner_idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2811, 106])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cyner_train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cyner_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ok\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.9 ('nlpproject_py312')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0af6a56369cba92bb59bd2db7ebca9b2b7c118936f54fea9e113554e0538962b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
