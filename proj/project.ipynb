{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project NLP and Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Project proposal presentation\n",
    "\n",
    "In the presentation, you have 5 minutes to present your research proposal. During the presentation, you should explain:\n",
    "\n",
    "* What is the topic of your project, what is the current state of this topic/task/setup\n",
    "* What is the new part of your project\n",
    "* What is the research question of your project\n",
    "\n",
    "We have proposed a number of topics in the slides which can be found on LearnIt, you can either pick one of these or come up with your own. If you pick your own, we suggest to get a pre-approval with Rob van der Goot.\n",
    "\n",
    "**Deadline for uploading slides: day before the presentation (23:59)**  (pdf only, they will be put into one long pdf for a smooth presentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Baseline\n",
    "To get your project started, you start with implementing a baseline model. Ideally, this is going to be the main baseline that you are going to compare to in your paper. Note that this baseline should be more advanced than just predicting the majority class (O).\n",
    "\n",
    "We will use EWT portion of the [Universal NER project](http://www.universalner.org/), which we provide with this notebook for convenience. You can use the train data (`en_ewt-ud-train.iob2`) and dev data(`en_ewt-ud-dev.iob2`) to build your baseline, then upload your prediction on the test data (`en_ewt-ud-test.iob2`).\n",
    "\n",
    "It is important to upload your predictions in same format as the training and dev files, so that the `span_f1.py` script can be used.\n",
    "\n",
    "Note that you do not have to implement your baseline from scratch, you can use for example the code from the RNN or BERT assignments as a starting point.\n",
    "\n",
    "**Deadline: 20-03 on LearnIt (14:00)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uname --nodename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import token\n",
    "import torch\n",
    "\n",
    "\n",
    "def read_iob2_file(path, sep=\"\\t\", word_index=1, tag_index=2):\n",
    "    \"\"\"\n",
    "    read in conll file\n",
    "\n",
    "    :param path: path to read from\n",
    "    :returns: list with sequences of words and labels for each sentence\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    current_words = []\n",
    "    current_tags = []\n",
    "\n",
    "    for line in open(path, encoding=\"utf-8\"):\n",
    "        line = line.strip()\n",
    "\n",
    "        if line:\n",
    "            if line[0] == \"#\":\n",
    "                continue  # skip comments\n",
    "            tok = line.split(sep)\n",
    "\n",
    "            current_words.append(tok[word_index])\n",
    "            current_tags.append(tok[tag_index])\n",
    "        else:\n",
    "            if current_words:  # skip empty lines\n",
    "                data.append((current_words, current_tags))\n",
    "            current_words = []\n",
    "            current_tags = []\n",
    "\n",
    "    # check for last one\n",
    "    if current_tags != []:\n",
    "        data.append((current_words, current_tags))\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_cyner(path, sep=\"\\t\", word_index=1, tag_index=2):\n",
    "    \"\"\"\n",
    "    read in conll file\n",
    "\n",
    "    :param path: path to read from\n",
    "    :returns: list with sequences of words and labels for each sentence\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    current_words = []\n",
    "    current_tags = []\n",
    "\n",
    "    for line in open(path, encoding=\"utf-8\"):\n",
    "        line = line.strip()\n",
    "\n",
    "        if line:\n",
    "            if line[0] == \"#\":\n",
    "                continue  # skip comments\n",
    "            tok = line.split(sep)\n",
    "\n",
    "            current_words.append(tok[word_index])\n",
    "            tag = tok[tag_index]\n",
    "            if \"Vulnerability\" in tag:\n",
    "                current_tags.append(\"Vulnerability\")\n",
    "            else:\n",
    "                current_tags.append(\"O\")\n",
    "        else:\n",
    "            if current_words:  # skip empty lines\n",
    "                data.append((current_words, current_tags))\n",
    "            current_words = []\n",
    "            current_tags = []\n",
    "\n",
    "    # check for last one\n",
    "    if current_tags != []:\n",
    "        data.append((current_words, current_tags))\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_aptner(path, sep=\" \", word_index=0, tag_index=1):\n",
    "    \"\"\"\n",
    "    read in conll file\n",
    "\n",
    "    :param path: path to read from\n",
    "    :returns: list with sequences of words and labels for each sentence\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    current_words = []\n",
    "    current_tags = []\n",
    "\n",
    "    for line in open(path, encoding=\"utf-8\"):\n",
    "        line = line.strip()\n",
    "\n",
    "        if line:\n",
    "            if line[0] == \"#\":\n",
    "                continue  # skip comments\n",
    "            tok = line.split(sep)\n",
    "\n",
    "            current_words.append(tok[word_index])\n",
    "            if len(tok) >= 2:\n",
    "                tag = tok[tag_index]\n",
    "                if \"B-VULNAME\" in tag:\n",
    "                    current_tags.append(\"B-VULNAME\")\n",
    "                elif \"I-VULNAME\" in tag or \"E-VULNAME\" in tag:\n",
    "                    current_tags.append(\"I-VULNAME\")\n",
    "            elif len(tok) == 1:\n",
    "                current_tags.append(\"O\")\n",
    "        else:\n",
    "            if current_words:  # skip empty lines\n",
    "                data.append((current_words, current_tags))\n",
    "            current_words = []\n",
    "            current_tags = []\n",
    "\n",
    "    # check for last one\n",
    "    if current_tags != []:\n",
    "        data.append((current_words, current_tags))\n",
    "    return data\n",
    "\n",
    "\n",
    "import jsonlines\n",
    "\n",
    "\n",
    "def read_attacker(path, sep=\" \", word_index=0, tag_index=1):\n",
    "    \"\"\"\n",
    "    read in conll file\n",
    "\n",
    "    :param path: path to read from\n",
    "    :returns: list with sequences of words and labels for each sentence\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    with jsonlines.open(path) as reader:\n",
    "        for obj in reader:\n",
    "            tags = [tag if \"VULNERABILITY\" in tag else \"O\" for tag in obj[\"tags\"]]\n",
    "            tokens = obj[\"tokens\"]\n",
    "            data.append((tokens, tags))\n",
    "    return data\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, pad_unk=\"<PAD>\"):\n",
    "        \"\"\"\n",
    "        A convenience class that can help store a vocabulary\n",
    "        and retrieve indices for inputs.\n",
    "        \"\"\"\n",
    "        self.pad_unk = pad_unk\n",
    "        self.word2idx = {self.pad_unk: 0}\n",
    "        self.idx2word = [self.pad_unk]\n",
    "\n",
    "    def getIdx(self, word, add=False):\n",
    "        if word not in self.word2idx:\n",
    "            if add:\n",
    "                self.word2idx[word] = len(self.idx2word)\n",
    "                self.idx2word.append(word)\n",
    "            else:\n",
    "                return self.word2idx[self.pad_unk]\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def getWord(self, idx):\n",
    "        return self.idx2word[idx]\n",
    "\n",
    "\n",
    "# Your implementation goes here:\n",
    "\n",
    "\n",
    "class Preprocess:\n",
    "    \"\"\"\n",
    "    data: the dataset from which we get the matrix used by a Neural network (instances + their tags)\n",
    "    instances: number of instances in the dataset, needed for dimension of matrix\n",
    "    features: the number of features/columns of the matrix\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.vocab_words = Vocab()\n",
    "        self.vocab_tags = Vocab()\n",
    "\n",
    "    def build_vocab(self, data, instances, features):\n",
    "        data_X = torch.zeros(instances, features, dtype=int)\n",
    "        data_y = torch.zeros(instances, features, dtype=int)\n",
    "        for i, sentence_tags in enumerate(data):\n",
    "            for j, word in enumerate(sentence_tags[0]):\n",
    "                data_X[i, j] = self.vocab_words.getIdx(word=word, add=True)\n",
    "                data_y[i, j] = self.vocab_tags.getIdx(\n",
    "                    word=sentence_tags[1][j], add=True\n",
    "                )\n",
    "\n",
    "        # returns the list of unique words in the list from the attributes of the Vocab()\n",
    "        idx2word_train = self.vocab_words.idx2word\n",
    "        # returns the list of unique tags in the list from the attributes of the Vocab()\n",
    "        idx2label_train = self.vocab_tags.idx2word\n",
    "        # only returned in the builder function, because they are reused for dev data in transform_prep_data()\n",
    "        return data_X, data_y, idx2word_train, idx2label_train\n",
    "\n",
    "    def transform_prep_data(self, data, instances, features):\n",
    "        # to be used only on dev data\n",
    "        data_X = torch.zeros(instances, features, dtype=int)\n",
    "        data_y = torch.zeros(instances, features, dtype=int)\n",
    "        for i, sentence_tags in enumerate(data):\n",
    "            for j, word in enumerate(sentence_tags[0]):\n",
    "                data_X[i, j] = self.vocab_words.getIdx(word=word, add=False)\n",
    "                data_y[i, j] = self.vocab_tags.getIdx(\n",
    "                    word=sentence_tags[1][j], add=False\n",
    "                )\n",
    "        return data_X, data_y\n",
    "\n",
    "\n",
    "def prepare_output_file(\n",
    "    transformer: Preprocess,\n",
    "    data: list,\n",
    "    pred_labels: torch.Tensor,\n",
    "    input_file: str,\n",
    "    output_file: str,\n",
    "):\n",
    "    global_labels = []\n",
    "    for (_, placeholder), labels_idxs in zip(data, pred_labels):\n",
    "        labels = []\n",
    "\n",
    "        for i in range(len(placeholder)):\n",
    "            labels.append(transformer.vocab_tags.idx2word[labels_idxs[i]])\n",
    "        global_labels += labels\n",
    "\n",
    "    with (\n",
    "        open(output_file, mode=\"w\", encoding=\"utf-8\") as f_out,\n",
    "        open(input_file, mode=\"r\", encoding=\"utf-8\") as f_in,\n",
    "    ):\n",
    "        i = 0\n",
    "        for line in f_in.readlines():\n",
    "            if line.strip():\n",
    "                if line[0] == \"#\":\n",
    "                    f_out.write(line)\n",
    "                else:\n",
    "                    words = line.split(\"\\t\")\n",
    "                    words[2] = global_labels[i]\n",
    "                    i += 1\n",
    "\n",
    "                    new_line = \"\\t\".join(words)\n",
    "                    f_out.write(new_line)\n",
    "            else:\n",
    "                f_out.write(\"\\n\")\n",
    "    assert i == len(global_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_iob2_file(\"./en_ewt-ud-train.iob2\")\n",
    "dev_data = read_iob2_file(\"./en_ewt-ud-dev.iob2\")\n",
    "test_data = read_iob2_file(\"./en_ewt-ud-test-masked.iob2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_iob2_file(\n",
    "    \"./data/APTNer/APTNERtrain.txt\", sep=\" \", word_index=0, tag_index=1\n",
    ")\n",
    "dev_data = read_iob2_file(\n",
    "    \"./data/APTNer/APTNERdev.txt\", sep=\" \", word_index=0, tag_index=1\n",
    ")\n",
    "test_data = read_iob2_file(\n",
    "    \"./data/APTNer/APTNERtest.txt\", sep=\" \", word_index=0, tag_index=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_attacker(\"./data/attackner/train.json\")\n",
    "dev_data = read_attacker(\"./data/attackner/dev.json\")\n",
    "test_data = read_attacker(\"./data/attackner/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_cyner(\"./data/cyner/train.txt\", word_index=0, tag_index=1)\n",
    "dev_data = read_cyner(\"./data/cyner/valid.txt\", word_index=0, tag_index=1)\n",
    "test_data = read_cyner(\"./data/cyner/test.txt\", word_index=0, tag_index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 106 is out of bounds for dimension 1 with size 106",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 8\u001b[0m\n\u001b[0;32m      2\u001b[0m max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m([\u001b[38;5;28mlen\u001b[39m(x[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m train_data])\n\u001b[0;32m      4\u001b[0m train_X, train_y, idx2word, idx2label \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mbuild_vocab(\n\u001b[0;32m      5\u001b[0m     train_data, \u001b[38;5;28mlen\u001b[39m(train_data), max_len\n\u001b[0;32m      6\u001b[0m )\n\u001b[1;32m----> 8\u001b[0m dev_X, dev_y \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform_prep_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdev_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m test_X, _ \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mtransform_prep_data(test_data, \u001b[38;5;28mlen\u001b[39m(test_data), max_len)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# here, the second variable doesn't hold true labels, as this is a test set. We need only to know the length of the sentences.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[51], line 195\u001b[0m, in \u001b[0;36mPreprocess.transform_prep_data\u001b[1;34m(self, data, instances, features)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, sentence_tags \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data):\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sentence_tags[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m--> 195\u001b[0m         \u001b[43mdata_X\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_words\u001b[38;5;241m.\u001b[39mgetIdx(word\u001b[38;5;241m=\u001b[39mword, add\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    196\u001b[0m         data_y[i, j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_tags\u001b[38;5;241m.\u001b[39mgetIdx(\n\u001b[0;32m    197\u001b[0m             word\u001b[38;5;241m=\u001b[39msentence_tags[\u001b[38;5;241m1\u001b[39m][j], add\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    198\u001b[0m         )\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_X, data_y\n",
      "\u001b[1;31mIndexError\u001b[0m: index 106 is out of bounds for dimension 1 with size 106"
     ]
    }
   ],
   "source": [
    "transformer = Preprocess()\n",
    "max_len = max([len(x[0]) for x in train_data])\n",
    "\n",
    "train_X, train_y, idx2word, idx2label = transformer.build_vocab(\n",
    "    train_data, len(train_data), max_len\n",
    ")\n",
    "\n",
    "dev_X, dev_y = transformer.transform_prep_data(dev_data, len(dev_data), max_len)\n",
    "\n",
    "test_X, _ = transformer.transform_prep_data(test_data, len(test_data), max_len)\n",
    "# here, the second variable doesn't hold true labels, as this is a test set. We need only to know the length of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put already to gpu if having space:\n",
    "train_X, train_y = train_X.to(device), train_y.to(device)\n",
    "dev_X, dev_y = dev_X.to(device), dev_y.to(device)\n",
    "test_X = test_X.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# TODO: Maybe dtype would need to be changed!\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = TensorDataset(train_X, train_y)\n",
    "train_loader = DataLoader(train_dataset, BATCH_SIZE)  # drop_last=True\n",
    "n_batches = len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "DIM_EMBEDDING = 100\n",
    "LSTM_HIDDEN = 100\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 15\n",
    "\n",
    "\n",
    "class TaggerModel(torch.nn.Module):\n",
    "    def __init__(self, nwords, ntags):\n",
    "        super().__init__()\n",
    "        # TODO Do Bidirectional LSTM\n",
    "        self.embed = nn.Embedding(nwords, DIM_EMBEDDING)\n",
    "        self.drop1 = nn.Dropout(p=0.2)\n",
    "        self.rnn = nn.LSTM(\n",
    "            DIM_EMBEDDING, LSTM_HIDDEN, batch_first=True, bidirectional=True\n",
    "        )\n",
    "        self.drop2 = nn.Dropout(p=0.3)\n",
    "        self.fc = nn.Linear(LSTM_HIDDEN * 2, ntags)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        word_vectors = self.embed(input_data)\n",
    "        regular1 = self.drop1(word_vectors)\n",
    "        output, hidden = self.rnn(regular1)\n",
    "        regular2 = self.drop2(output)\n",
    "\n",
    "        predictions = self.fc(regular2)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "model = TaggerModel(len(idx2word), len(idx2label))\n",
    "model = model.to(device)  # run on cuda if possible\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=0, reduction=\"sum\")\n",
    "\n",
    "# creating the batches\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    # reset the gradient\n",
    "    print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "    loss_sum = 0\n",
    "\n",
    "    # loop over batches\n",
    "    # types for convenience\n",
    "    batch_X: torch.Tensor\n",
    "    batch_y: torch.Tensor\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # TODO: if having memory issues comment .to(device)\n",
    "        # from one of the previous cells, and uncomment that:\n",
    "        # batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predicted_values = model.forward(batch_X)\n",
    "\n",
    "        # Cross entropy request (predictions, classes) shape for predictions, and (classes) for batch_y\n",
    "\n",
    "        # calculate loss\n",
    "        loss = loss_function(\n",
    "            predicted_values.view(batch_X.shape[0] * max_len, -1), batch_y.flatten()\n",
    "        )  # TODO: Last batch has 31 entries instead of 32 - we don't adjust much for that.\n",
    "        loss_sum += loss.item()  # avg later\n",
    "\n",
    "        # update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Average loss after epoch {epoch + 1}: {loss_sum / n_batches}\")\n",
    "\n",
    "# set to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# eval using Span_F1\n",
    "predictions_dev = model.forward(dev_X)\n",
    "print(predictions_dev.shape)\n",
    "# gives probabilities for each tag (dim=18) for each word/feature (dim=159) for each sentence(dim=2000)\n",
    "# we want to classify each word for the part-of-speech with highest probability\n",
    "labels_dev = torch.argmax(predictions_dev, 2)\n",
    "print(labels_dev.shape)\n",
    "prepare_output_file(\n",
    "    transformer, dev_data, labels_dev, \"./en_ewt-ud-dev.iob2\", \"./dev.iob2\"\n",
    ")\n",
    "\n",
    "del predictions_dev\n",
    "del labels_dev\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "!python span_f1.py en_ewt-ud-dev.iob2 dev.iob2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Eval using just accuracy.\n",
    "\n",
    "# labels_dev = torch.flatten(labels_dev)  # model predictions\n",
    "# dev_y_flat = torch.flatten(dev_y)  # true labels\n",
    "# acc = []\n",
    "# for i in range(len(labels_dev)):\n",
    "#     if dev_y_flat[i] != 0:\n",
    "#         acc.append(int(labels_dev[i] == dev_y_flat[i]))\n",
    "\n",
    "# accuracy = sum(acc) / len(acc)\n",
    "# print(f\"Model accuracy on dev set: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save test for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Evaluating on dev data we will predict using trained TaggerModel\n",
    "predictions_test = model.forward(test_X)\n",
    "print(predictions_test.shape)\n",
    "# gives probabilities for each tag (dim=18) for each word/feature (dim=159) for each sentence(dim=2000)\n",
    "# we want to classify each word for the part-of-speech with highest probability\n",
    "labels_test = torch.argmax(predictions_test, 2)\n",
    "print(labels_test.shape)\n",
    "### save labels\n",
    "prepare_output_file(\n",
    "    transformer, test_data, labels_test, \"./en_ewt-ud-test-masked.iob2\", \"./test.iob2\"\n",
    ")\n",
    "\n",
    "del predictions_test\n",
    "del labels_test\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Project proposal\n",
    "\n",
    "The written proposal should consist of maximum one page in [ACL-format](https://github.com/acl-org/acl-style-files) (The bibliography does not count for the word limit). In here, you should explain the last three points from the list above and place your project in a larger context (previous work).\n",
    "\n",
    "Make sure your proposal is:\n",
    "* Novel to some extent\n",
    "* Doable within the time-frame\n",
    "\n",
    "*hint* The [ACL Anthology](https://aclanthology.org/) contains almost all peer-reviewed NLP papers.\n",
    "\n",
    "**Deadline: 03-04 on LearnIt (14:00)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Final project\n",
    "The final project has a maximum size of 5 pages (excluding bibliography and appendix), using the [ACL style files](https://github.com/acl-org/acl-style-files)\n",
    "\n",
    "Besides the main paper (discussed in class), you have to include:\n",
    "* Group contributions. State who was responsible for which part of the project. Here you may state if there\n",
    "were any serious unequal workloads among group members. This should be put in the appendix.\n",
    "* A report on usage of chatbots. We follow: https://2023.aclweb.org/blog/ACL-2023-policy/\n",
    "   * Add a section in appendix if you made use of a chatbot (since we do not use a Responsible NLP Checklist)\n",
    "   * Include each stage on the ACL policy, and indicate to what extent you used a chatbot\n",
    "   * Use with care!, you are responsible for the project and plagiarism, correctness etc.\n",
    "\n",
    "You can also put additional results and details in the appendix. However, the paper itself should be standalone, and understandable without consulting the appendix.\n",
    "\n",
    "Furthermore, the code should be available on www.github.itu.dk (with a link in a footnote at the end of the abstract) , it should include a README with instructions on how to reproduce your results.\n",
    "\n",
    "**Deadline: 23-05 on LearnIt** Please check the checklist below before uploading!\n",
    "\n",
    "Optionally, you can upload a draft a week before **16-05 (before 09:00)** for an extra round of feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Analysis is essential for the interpretation of your results. In this section we will shortly describe some different types of analysis. We strongly suggest to use at least one of these:\n",
    "\n",
    "* **Ablation study**: Leave out a certain part of the model, to study its effects. For example, disable the tokenizer, remove a certain (group of) feature(s), or disable the stop-word removal. If the performance drops a lot, it means that this part of the model contributes heavily to the models final performance. This is commonly done in 1 table, while disabling different parts of the model. Note that you can also do this the other way around, i.e. use only one feature (group) at a time, and test performance\n",
    "* **Learning curve**: Evaluate how much data your model needs to reach a certain performance. Especially for the data augmentation projects this is essential.\n",
    "* **Quantitative analysis**: Automated means of analyzing in which cases your model performs worse. This can for example be done with a confusion matrix.\n",
    "* **Qualitative analysis**: Manually inspect a certain number of errors, and try to categorize them/find trends. Can be combined with the quantitative analysis, i.e., inspect 100 cases of positive reviews predicted to be negative and 100 cases of negative reviews predicted to be positive\n",
    "* **Feature importance**: In traditional machine learning methods, one can often extract and inspect the weights of the features. In sklearn these can be found in: `trained_model.coef_`\n",
    "* **Other metrics**: per class scores, partial matches, or count how often the span-borders were correct, but the label wrong.\n",
    "* **Input words importance**: To gain insight into which words have a impact on prediction performance (positive, negative), we can analyze per-word impact: given a trained model, replace a given word with\n",
    "the unknown word token and observe the change in prediction score (probability for a class). This is\n",
    "shown in Figure 4 of [Rethmeier et al (2018)](https://aclweb.org/anthology/W18-6246) (a paper on controversy detection), also shown below: red-colored\n",
    "tokens were important for controversy detection, blue-colored token decreased prediction scores.\n",
    "\n",
    "<img width=400px src=example.png>\n",
    "\n",
    "Note that this is a non-exhaustive list, and you are encouraged to also explore additional analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checklist final project\n",
    "Please check all these items before handing in your final report. You only have to upload a pdf file on learnit, and make sure a link to the code is included in the report and the code is accesible. \n",
    "\n",
    "* Are all group members and their email addresses specified?\n",
    "* Does the group report include a representative project title?\n",
    "* Does the group report contain an abstract?\n",
    "* Does the introduction clearly specify the research intention and research question?\n",
    "* Does the group report adequately refer to the relevant literature?\n",
    "* Does the group report properly use figure, tables and examples?\n",
    "* Does the group report provide and discuss the empirical results?\n",
    "* Is the group report proofread?\n",
    "* Does the pdf contain the link to the projectâ€™s github repo?\n",
    "* Is the github repo accessible to the public (within ITU)?\n",
    "* Is the group report maximum 5 pages long, excluding references and appendix?\n",
    "* Are the group contributions added in the appendix?\n",
    "* Does the repository contain all scripts and code to reproduce the results in the group report? Are instructions\n",
    " provided on how to run the code?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
